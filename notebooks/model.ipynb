{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Model\n",
    "Here we will be creating different models to classify our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set configurations\n",
    "pd.set_option('display.max_columns', 100)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# model imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from PIL import Image\n",
    "import wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.read_csv('../data/data_for_model.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_suicide</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>selftext_length</th>\n",
       "      <th>title_length</th>\n",
       "      <th>megatext_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our most-broken and least-understood rules is ...</td>\n",
       "      <td>We understand that most people who reply immed...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/d...</td>\n",
       "      <td>understand people reply immediately op invitat...</td>\n",
       "      <td>broken least understood rule helper may invite...</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>4792</td>\n",
       "      <td>144</td>\n",
       "      <td>sql witch understand people reply immediately ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Regular Check-In Post</td>\n",
       "      <td>Welcome to /r/depression's check-in post - a p...</td>\n",
       "      <td>circinia</td>\n",
       "      <td>1644</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/e...</td>\n",
       "      <td>welcome r depression check post place take mom...</td>\n",
       "      <td>regular check post</td>\n",
       "      <td>c irc</td>\n",
       "      <td>650</td>\n",
       "      <td>21</td>\n",
       "      <td>c irc welcome r depression check post place ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I hate it so much when you try and express you...</td>\n",
       "      <td>I've been feeling really depressed and lonely ...</td>\n",
       "      <td>TheNewKiller69</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/f...</td>\n",
       "      <td>feeling really depressed lonely lately job ful...</td>\n",
       "      <td>hate much try express feeling parent turn arou...</td>\n",
       "      <td>new killer 69</td>\n",
       "      <td>1866</td>\n",
       "      <td>137</td>\n",
       "      <td>new killer 69 feeling really depressed lonely ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Our most-broken and least-understood rules is ...   \n",
       "1                              Regular Check-In Post   \n",
       "2  I hate it so much when you try and express you...   \n",
       "\n",
       "                                            selftext          author  \\\n",
       "0  We understand that most people who reply immed...        SQLwitch   \n",
       "1  Welcome to /r/depression's check-in post - a p...        circinia   \n",
       "2  I've been feeling really depressed and lonely ...  TheNewKiller69   \n",
       "\n",
       "   num_comments  is_suicide  \\\n",
       "0           133           0   \n",
       "1          1644           0   \n",
       "2             8           0   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.reddit.com/r/depression/comments/d...   \n",
       "1  https://www.reddit.com/r/depression/comments/e...   \n",
       "2  https://www.reddit.com/r/depression/comments/f...   \n",
       "\n",
       "                                      selftext_clean  \\\n",
       "0  understand people reply immediately op invitat...   \n",
       "1  welcome r depression check post place take mom...   \n",
       "2  feeling really depressed lonely lately job ful...   \n",
       "\n",
       "                                         title_clean   author_clean  \\\n",
       "0  broken least understood rule helper may invite...      sql witch   \n",
       "1                                 regular check post          c irc   \n",
       "2  hate much try express feeling parent turn arou...  new killer 69   \n",
       "\n",
       "   selftext_length  title_length  \\\n",
       "0             4792           144   \n",
       "1              650            21   \n",
       "2             1866           137   \n",
       "\n",
       "                                      megatext_clean  \n",
       "0  sql witch understand people reply immediately ...  \n",
       "1  c irc welcome r depression check post place ta...  \n",
       "2  new killer 69 feeling really depressed lonely ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1897 entries, 0 to 1896\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   title            1897 non-null   object\n",
      " 1   selftext         1897 non-null   object\n",
      " 2   author           1897 non-null   object\n",
      " 3   num_comments     1897 non-null   int64 \n",
      " 4   is_suicide       1897 non-null   int64 \n",
      " 5   url              1897 non-null   object\n",
      " 6   selftext_clean   1897 non-null   object\n",
      " 7   title_clean      1897 non-null   object\n",
      " 8   author_clean     1897 non-null   object\n",
      " 9   selftext_length  1897 non-null   int64 \n",
      " 10  title_length     1897 non-null   int64 \n",
      " 11  megatext_clean   1897 non-null   object\n",
      "dtypes: int64(4), object(8)\n",
      "memory usage: 178.0+ KB\n"
     ]
    }
   ],
   "source": [
    "model_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establishing a baseline accuracy is important for evaluating the model's progression. If every prediction was 1, let's see what our accuracy would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5166051660516605"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data['is_suicide'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline accuracy is about 51.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of Features and Attributes\n",
    "Here we are attempting to create a model using CountVectorizer and Naive Bayes Model to determine which columns are the best to score and use for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to score different models from different columsn in the dataset\n",
    "\n",
    "columns_list = ['column_1', \"column_2\", \"column_3\"]\n",
    "model = \"CountVec + MultinomialNB\"\n",
    "df_list=[] \n",
    "\n",
    "def multi_modelling(columns_list, model):\n",
    "    for i in columns_list:\n",
    "        \n",
    "        X = model_data[i]\n",
    "        y = model_data['is_suicide']\n",
    "        \n",
    "        # train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "        \n",
    "        cvec = CountVectorizer()\n",
    "        cvec.fit(X_train)\n",
    "        \n",
    "        # creating dataframes\n",
    "        X_train = pd.DataFrame(cvec.transform(X_train).todense(),\n",
    "                               columns=cvec.get_feature_names())\n",
    "        X_test = pd.DataFrame(cvec.transform(X_test).todense(),\n",
    "                               columns=cvec.get_feature_names())\n",
    "        \n",
    "        nb = MultinomialNB()\n",
    "        nb.fit(X_train,y_train)\n",
    "        \n",
    "        # get predictions from model\n",
    "        pred = nb.predict(X_test)\n",
    "        \n",
    "        # create confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "        \n",
    "        # calculating AUC\n",
    "        nb.predict_proba(X_test)\n",
    "        pred_proba = [i[1] for i in nb.predict_proba(X_test)] \n",
    "        auc = roc_auc_score(y_test, pred_proba)\n",
    "\n",
    "        # classification report\n",
    "        classi_dict = (classification_report(y_test,pred, output_dict=True))\n",
    "\n",
    "        # storing our results\n",
    "        model_results = {}\n",
    "        model_results['series used (X)'] = i\n",
    "        model_results['model'] = model\n",
    "        model_results['AUC Score'] = auc\n",
    "        model_results['precision']= classi_dict['weighted avg']['precision']\n",
    "        model_results['recall (sensitivity)']= classi_dict['weighted avg']['recall']\n",
    "        model_results['confusion matrix']={\"TP\": tp,\"FP\":fp, \"TN\": tn, \"FN\": fn}\n",
    "        model_results['train accuracy'] = nb.score(X_train, y_train)\n",
    "        model_results['test accuracy'] = nb.score(X_test, y_test)\n",
    "        model_results['baseline accuracy']=0.5166\n",
    "        model_results['specificity']= tn/(tn+fp)  \n",
    "        model_results['f1-score']= classi_dict['weighted avg']['f1-score']\n",
    "        #model_results['support']= classi_dict['weighted avg']['support']\n",
    "        model_results\n",
    "        df_list.append(model_results) \n",
    "\n",
    "    pd.set_option(\"display.max_colwidth\", 50)\n",
    "    return (pd.DataFrame(df_list)).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our initial model, it will be a binary classifier. Once the user gets their classification, they will go to another model that will give them specific support. \n",
    "\n",
    "Label Encoding: r/SuicideWatch = 1, r/Depression = 0\n",
    "\n",
    "TP: model predicts suicide, and it is correct\n",
    "\n",
    "TN: model predicts depression, and it is correct\n",
    "\n",
    "FP: model predicts suicide, but it is really depression, not good\n",
    "\n",
    "FN: model predicts depression, but they really are suicidal, this is the worst, misses an at risk patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series used (X)</th>\n",
       "      <th>model</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall (sensitivity)</th>\n",
       "      <th>confusion matrix</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>baseline accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>selftext</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>{'TP': 161, 'FP': 78, 'TN': 152, 'FN': 84}</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>author</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.55</td>\n",
       "      <td>{'TP': 235, 'FP': 204, 'TN': 26, 'FN': 10}</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>title</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'TP': 167, 'FP': 104, 'TN': 126, 'FN': 78}</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>selftext_clean</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'TP': 165, 'FP': 78, 'TN': 152, 'FN': 80}</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>author_clean</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.51</td>\n",
       "      <td>{'TP': 169, 'FP': 155, 'TN': 75, 'FN': 76}</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>title_clean</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'TP': 178, 'FP': 112, 'TN': 118, 'FN': 67}</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>megatext_clean</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  series used (X)                     model  AUC Score  precision  \\\n",
       "0        selftext  CountVec + MultinomialNB       0.69       0.66   \n",
       "1          author  CountVec + MultinomialNB       0.57       0.63   \n",
       "2           title  CountVec + MultinomialNB       0.67       0.62   \n",
       "3  selftext_clean  CountVec + MultinomialNB       0.69       0.67   \n",
       "4    author_clean  CountVec + MultinomialNB       0.54       0.51   \n",
       "5     title_clean  CountVec + MultinomialNB       0.67       0.63   \n",
       "6  megatext_clean  CountVec + MultinomialNB       0.71       0.67   \n",
       "\n",
       "   recall (sensitivity)                             confusion matrix  \\\n",
       "0                  0.66   {'TP': 161, 'FP': 78, 'TN': 152, 'FN': 84}   \n",
       "1                  0.55   {'TP': 235, 'FP': 204, 'TN': 26, 'FN': 10}   \n",
       "2                  0.62  {'TP': 167, 'FP': 104, 'TN': 126, 'FN': 78}   \n",
       "3                  0.67   {'TP': 165, 'FP': 78, 'TN': 152, 'FN': 80}   \n",
       "4                  0.51   {'TP': 169, 'FP': 155, 'TN': 75, 'FN': 76}   \n",
       "5                  0.62  {'TP': 178, 'FP': 112, 'TN': 118, 'FN': 67}   \n",
       "6                  0.67   {'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}   \n",
       "\n",
       "   train accuracy  test accuracy  baseline accuracy  specificity  f1-score  \n",
       "0            0.92           0.66               0.52         0.66      0.66  \n",
       "1            0.99           0.55               0.52         0.11      0.45  \n",
       "2            0.85           0.62               0.52         0.55      0.62  \n",
       "3            0.91           0.67               0.52         0.66      0.67  \n",
       "4            0.95           0.51               0.52         0.33      0.50  \n",
       "5            0.84           0.62               0.52         0.51      0.62  \n",
       "6            0.95           0.67               0.52         0.69      0.67  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the function\n",
    "columns_list = ['selftext', \"author\", \"title\",'selftext_clean', \"author_clean\", \"title_clean\", \"megatext_clean\"]\n",
    "model = \"CountVec + MultinomialNB\"\n",
    "df_list=[]\n",
    "multi_modelling(columns_list, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our results, using the megatext_clean column will be the best option for our model due its good metrics, such as its AUC value. The false negatives are the highest in our confusion matrix, meaning it is good for the model to learn from. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Model\n",
    "We will test a few different models by running some perumatations and recording the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that test different models and record parameters\n",
    "\n",
    "def gridsearch_multi(steps_titles, steps_list, pipe_params):\n",
    "    \n",
    "    X = model_data[\"megatext_clean\"]\n",
    "    y = model_data['is_suicide']\n",
    "    \n",
    "    # train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "    # build dataframe\n",
    "    gs_results = pd.DataFrame(columns=['model','AUC Score', 'precision', 'recall (sensitivity)', \n",
    "                                       'best_params', 'best score', 'confusion matrix', \n",
    "                                       'train_accuracy','test_accuracy','baseline_accuracy',\n",
    "                                       'specificity', 'f1-score'])\n",
    "\n",
    "    for i in range(len(steps_list)):\n",
    "        # define pipeline\n",
    "        pipe = Pipeline(steps=steps_list[i])\n",
    "\n",
    "        gs = GridSearchCV(pipe, pipe_params[i], cv=3) \n",
    "        gs.fit(X_train, y_train)\n",
    "        \n",
    "        # getting predictions from model\n",
    "        pred = gs.predict(X_test)\n",
    "        \n",
    "        # confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, gs.predict(X_test)).ravel() \n",
    "        \n",
    "        classi_dict = (classification_report(y_test,pred, output_dict=True))\n",
    "        \n",
    "        # calculating AUC\n",
    "        gs.predict_proba(X_test)\n",
    "        pred_proba = [i[1] for i in gs.predict_proba(X_test)] \n",
    "        auc = roc_auc_score(y_test, pred_proba)\n",
    "        \n",
    "        # create dataframe columns\n",
    "        model_results = {}\n",
    "        model_results['model'] = steps_titles[i]\n",
    "        model_results['AUC Score'] = auc\n",
    "        model_results['precision']= classi_dict['weighted avg']['precision']\n",
    "        model_results['recall (sensitivity)']= classi_dict['weighted avg']['recall']\n",
    "        model_results['best params'] = gs.best_params_\n",
    "        model_results['best score'] = gs.best_score_\n",
    "        model_results['confusion matrix']={\"TP\": tp,\"FP\":fp, \"TN\": tn, \"FN\": fn}\n",
    "        model_results['train accuracy'] = gs.score(X_train, y_train)\n",
    "        model_results['test accuracy'] = gs.score(X_test, y_test)\n",
    "        model_results['baseline accuracy'] = 0.5166\n",
    "        \n",
    "        model_results['specificity']= tn/(tn+fp)  \n",
    "        model_results['f1-score']= classi_dict['weighted avg']['f1-score']\n",
    "\n",
    "\n",
    "        # add data\n",
    "        df_list.append(model_results) \n",
    "        pd.set_option(\"display.max_colwidth\", 200)\n",
    "    return (pd.DataFrame(df_list)).round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall (sensitivity)</th>\n",
       "      <th>best params</th>\n",
       "      <th>best score</th>\n",
       "      <th>confusion matrix</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>baseline accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cvec+ multi_nb</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cvec + ss + knn</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>{'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  AUC Score  precision  recall (sensitivity)  \\\n",
       "0      cvec+ multi_nb       0.72       0.67                  0.67   \n",
       "1     cvec + ss + knn       0.60       0.58                  0.58   \n",
       "2  cvec + ss + logreg       0.73       0.69                  0.69   \n",
       "\n",
       "                                                                                                            best params  \\\n",
       "0  {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "1  {'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "2  {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "\n",
       "   best score                              confusion matrix  train accuracy  \\\n",
       "0        0.65    {'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}            0.68   \n",
       "1        0.60  {'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}            0.72   \n",
       "2        0.65    {'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}            0.69   \n",
       "\n",
       "   test accuracy  baseline accuracy  specificity  f1-score  \n",
       "0           0.67               0.52         0.69      0.67  \n",
       "1           0.58               0.52         0.57      0.58  \n",
       "2           0.69               0.52         0.67      0.69  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing function with count vectorizor\n",
    "\n",
    "df_list=[]\n",
    "\n",
    "# models\n",
    "steps_titles = ['cvec+ multi_nb','cvec + ss + knn','cvec + ss + logreg']\n",
    "\n",
    "# pipelines\n",
    "steps_list = [ \n",
    "    [('cv', CountVectorizer()),('multi_nb', MultinomialNB())],\n",
    "    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]\n",
    "\n",
    "# parameters\n",
    "pipe_params = [\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]},\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]},\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]}\n",
    "]   \n",
    "\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall (sensitivity)</th>\n",
       "      <th>best params</th>\n",
       "      <th>best score</th>\n",
       "      <th>confusion matrix</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>baseline accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cvec+ multi_nb</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cvec + ss + knn</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>{'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tvec + multi_nb</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tvec + ss + knn</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  AUC Score  precision  recall (sensitivity)  \\\n",
       "0      cvec+ multi_nb       0.72       0.67                  0.67   \n",
       "1     cvec + ss + knn       0.60       0.58                  0.58   \n",
       "2  cvec + ss + logreg       0.73       0.69                  0.69   \n",
       "3     tvec + multi_nb       0.73       0.68                  0.68   \n",
       "4     tvec + ss + knn       0.56       0.54                  0.54   \n",
       "5  tvec + ss + logreg       0.73       0.67                  0.67   \n",
       "\n",
       "                                                                                                            best params  \\\n",
       "0  {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "1  {'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "2  {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "3  {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "4  {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n",
       "5  {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "\n",
       "   best score                              confusion matrix  train accuracy  \\\n",
       "0        0.65    {'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}            0.68   \n",
       "1        0.60  {'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}            0.72   \n",
       "2        0.65    {'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}            0.69   \n",
       "3        0.65    {'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}            0.68   \n",
       "4        0.60   {'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}            0.74   \n",
       "5        0.65    {'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}            0.68   \n",
       "\n",
       "   test accuracy  baseline accuracy  specificity  f1-score  \n",
       "0           0.67               0.52         0.69      0.67  \n",
       "1           0.58               0.52         0.57      0.58  \n",
       "2           0.69               0.52         0.67      0.69  \n",
       "3           0.68               0.52         0.67      0.68  \n",
       "4           0.54               0.52         0.49      0.54  \n",
       "5           0.67               0.52         0.69      0.67  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing function with TFD Vectorizor\n",
    "\n",
    "# models\n",
    "steps_titles = ['tvec + multi_nb','tvec + ss + knn','tvec + ss + logreg']\n",
    "\n",
    "# pipelines\n",
    "steps_list = [ \n",
    "    [('tv', TfidfVectorizer()),('multi_nb', MultinomialNB())],\n",
    "    [('tv', TfidfVectorizer()),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('tv', TfidfVectorizer()),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]\n",
    "\n",
    "# parameters\n",
    "pipe_params = [\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]},\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]},\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]}\n",
    "]   \n",
    "\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall (sensitivity)</th>\n",
       "      <th>best params</th>\n",
       "      <th>best score</th>\n",
       "      <th>confusion matrix</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>baseline accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cvec+ multi_nb</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cvec + ss + knn</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>{'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tvec + multi_nb</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tvec + ss + knn</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hvec + multi_nb</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>{'TP': 148, 'FP': 54, 'TN': 176, 'FN': 97}</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hvec + ss + knn</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.52</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.52</td>\n",
       "      <td>{'TP': 245, 'FP': 229, 'TN': 1, 'FN': 0}</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hvec + ss + logreg</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.63</td>\n",
       "      <td>{'TP': 155, 'FP': 90, 'TN': 140, 'FN': 90}</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hvec + multi_nb(tuning)</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'hv__n_features': 1000, 'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'TP': 170, 'FP': 75, 'TN': 155, 'FN': 75}</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tvec + multi_nb(tuning)</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'tv__max_df': 0.4, 'tv__max_features': 70, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'TP': 172, 'FP': 76, 'TN': 154, 'FN': 73}</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hvec + multi_nb</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>{'TP': 148, 'FP': 54, 'TN': 176, 'FN': 97}</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hvec + ss + knn</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.52</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.52</td>\n",
       "      <td>{'TP': 245, 'FP': 229, 'TN': 1, 'FN': 0}</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hvec + ss + logreg</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.63</td>\n",
       "      <td>{'TP': 155, 'FP': 90, 'TN': 140, 'FN': 90}</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model  AUC Score  precision  recall (sensitivity)  \\\n",
       "0            cvec+ multi_nb       0.72       0.67                  0.67   \n",
       "1           cvec + ss + knn       0.60       0.58                  0.58   \n",
       "2        cvec + ss + logreg       0.73       0.69                  0.69   \n",
       "3           tvec + multi_nb       0.73       0.68                  0.68   \n",
       "4           tvec + ss + knn       0.56       0.54                  0.54   \n",
       "5        tvec + ss + logreg       0.73       0.67                  0.67   \n",
       "6           hvec + multi_nb       0.77       0.69                  0.68   \n",
       "7           hvec + ss + knn       0.51       0.75                  0.52   \n",
       "8        hvec + ss + logreg       0.65       0.62                  0.62   \n",
       "9   hvec + multi_nb(tuning)       0.75       0.68                  0.68   \n",
       "10  tvec + multi_nb(tuning)       0.75       0.69                  0.69   \n",
       "11          hvec + multi_nb       0.77       0.69                  0.68   \n",
       "12          hvec + ss + knn       0.51       0.75                  0.52   \n",
       "13       hvec + ss + logreg       0.65       0.62                  0.62   \n",
       "\n",
       "                                                                                                             best params  \\\n",
       "0   {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "1   {'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "2   {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "3   {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "4   {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n",
       "5   {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "6                                                               {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "7                                                               {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "8                                                               {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "9                                       {'hv__n_features': 1000, 'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "10  {'tv__max_df': 0.4, 'tv__max_features': 70, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "11                                                              {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "12                                                              {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "13                                                              {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "\n",
       "    best score                              confusion matrix  train accuracy  \\\n",
       "0         0.65    {'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}            0.68   \n",
       "1         0.60  {'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}            0.72   \n",
       "2         0.65    {'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}            0.69   \n",
       "3         0.65    {'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}            0.68   \n",
       "4         0.60   {'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}            0.74   \n",
       "5         0.65    {'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}            0.68   \n",
       "6         0.72    {'TP': 148, 'FP': 54, 'TN': 176, 'FN': 97}            0.89   \n",
       "7         0.52      {'TP': 245, 'FP': 229, 'TN': 1, 'FN': 0}            0.52   \n",
       "8         0.63    {'TP': 155, 'FP': 90, 'TN': 140, 'FN': 90}            1.00   \n",
       "9         0.69    {'TP': 170, 'FP': 75, 'TN': 155, 'FN': 75}            0.82   \n",
       "10        0.68    {'TP': 172, 'FP': 76, 'TN': 154, 'FN': 73}            0.71   \n",
       "11        0.72    {'TP': 148, 'FP': 54, 'TN': 176, 'FN': 97}            0.89   \n",
       "12        0.52      {'TP': 245, 'FP': 229, 'TN': 1, 'FN': 0}            0.52   \n",
       "13        0.63    {'TP': 155, 'FP': 90, 'TN': 140, 'FN': 90}            1.00   \n",
       "\n",
       "    test accuracy  baseline accuracy  specificity  f1-score  \n",
       "0            0.67               0.52         0.69      0.67  \n",
       "1            0.58               0.52         0.57      0.58  \n",
       "2            0.69               0.52         0.67      0.69  \n",
       "3            0.68               0.52         0.67      0.68  \n",
       "4            0.54               0.52         0.49      0.54  \n",
       "5            0.67               0.52         0.69      0.67  \n",
       "6            0.68               0.52         0.77      0.68  \n",
       "7            0.52               0.52         0.00      0.36  \n",
       "8            0.62               0.52         0.61      0.62  \n",
       "9            0.68               0.52         0.67      0.68  \n",
       "10           0.69               0.52         0.67      0.69  \n",
       "11           0.68               0.52         0.77      0.68  \n",
       "12           0.52               0.52         0.00      0.36  \n",
       "13           0.62               0.52         0.61      0.62  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing function with Hashing Vectorizor\n",
    "\n",
    "# models\n",
    "steps_titles = ['hvec + multi_nb','hvec + ss + knn','hvec + ss + logreg']\n",
    "\n",
    "# pipelines\n",
    "steps_list = [ \n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('multi_nb', MultinomialNB())],\n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]\n",
    "\n",
    "# parameters\n",
    "pipe_params = [\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]},\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]},\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]}\n",
    "]   \n",
    "\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing all the performances, two models stand out. The Hashing Vectorizor and Multinomial Naive Bayes model had much better metrics, along with the TFID Vectorizor and Multinomial Naive Bayes Model. They both have good AUC values and high generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall (sensitivity)</th>\n",
       "      <th>best params</th>\n",
       "      <th>best score</th>\n",
       "      <th>confusion matrix</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>baseline accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cvec+ multi_nb</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cvec + ss + knn</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>{'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tvec + multi_nb</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tvec + ss + knn</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hvec + multi_nb</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>{'TP': 148, 'FP': 54, 'TN': 176, 'FN': 97}</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hvec + ss + knn</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.52</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.52</td>\n",
       "      <td>{'TP': 245, 'FP': 229, 'TN': 1, 'FN': 0}</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hvec + ss + logreg</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.63</td>\n",
       "      <td>{'TP': 155, 'FP': 90, 'TN': 140, 'FN': 90}</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hvec + multi_nb(tuning)</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'hv__n_features': 1000, 'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'TP': 170, 'FP': 75, 'TN': 155, 'FN': 75}</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tvec + multi_nb(tuning)</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'tv__max_df': 0.4, 'tv__max_features': 70, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'TP': 172, 'FP': 76, 'TN': 154, 'FN': 73}</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hvec + multi_nb</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>{'TP': 148, 'FP': 54, 'TN': 176, 'FN': 97}</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hvec + ss + knn</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.52</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.52</td>\n",
       "      <td>{'TP': 245, 'FP': 229, 'TN': 1, 'FN': 0}</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hvec + ss + logreg</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.63</td>\n",
       "      <td>{'TP': 155, 'FP': 90, 'TN': 140, 'FN': 90}</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hvec + multi_nb(tuning)</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'hv__n_features': 1000, 'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'TP': 170, 'FP': 75, 'TN': 155, 'FN': 75}</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tvec + multi_nb(tuning)</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'tv__max_df': 0.4, 'tv__max_features': 70, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'TP': 172, 'FP': 76, 'TN': 154, 'FN': 73}</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model  AUC Score  precision  recall (sensitivity)  \\\n",
       "0            cvec+ multi_nb       0.72       0.67                  0.67   \n",
       "1           cvec + ss + knn       0.60       0.58                  0.58   \n",
       "2        cvec + ss + logreg       0.73       0.69                  0.69   \n",
       "3           tvec + multi_nb       0.73       0.68                  0.68   \n",
       "4           tvec + ss + knn       0.56       0.54                  0.54   \n",
       "5        tvec + ss + logreg       0.73       0.67                  0.67   \n",
       "6           hvec + multi_nb       0.77       0.69                  0.68   \n",
       "7           hvec + ss + knn       0.51       0.75                  0.52   \n",
       "8        hvec + ss + logreg       0.65       0.62                  0.62   \n",
       "9   hvec + multi_nb(tuning)       0.75       0.68                  0.68   \n",
       "10  tvec + multi_nb(tuning)       0.75       0.69                  0.69   \n",
       "11          hvec + multi_nb       0.77       0.69                  0.68   \n",
       "12          hvec + ss + knn       0.51       0.75                  0.52   \n",
       "13       hvec + ss + logreg       0.65       0.62                  0.62   \n",
       "14  hvec + multi_nb(tuning)       0.75       0.68                  0.68   \n",
       "15  tvec + multi_nb(tuning)       0.75       0.69                  0.69   \n",
       "\n",
       "                                                                                                             best params  \\\n",
       "0   {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "1   {'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "2   {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "3   {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "4   {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n",
       "5   {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "6                                                               {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "7                                                               {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "8                                                               {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "9                                       {'hv__n_features': 1000, 'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "10  {'tv__max_df': 0.4, 'tv__max_features': 70, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "11                                                              {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "12                                                              {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "13                                                              {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "14                                      {'hv__n_features': 1000, 'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "15  {'tv__max_df': 0.4, 'tv__max_features': 70, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "\n",
       "    best score                              confusion matrix  train accuracy  \\\n",
       "0         0.65    {'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}            0.68   \n",
       "1         0.60  {'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}            0.72   \n",
       "2         0.65    {'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}            0.69   \n",
       "3         0.65    {'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}            0.68   \n",
       "4         0.60   {'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}            0.74   \n",
       "5         0.65    {'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}            0.68   \n",
       "6         0.72    {'TP': 148, 'FP': 54, 'TN': 176, 'FN': 97}            0.89   \n",
       "7         0.52      {'TP': 245, 'FP': 229, 'TN': 1, 'FN': 0}            0.52   \n",
       "8         0.63    {'TP': 155, 'FP': 90, 'TN': 140, 'FN': 90}            1.00   \n",
       "9         0.69    {'TP': 170, 'FP': 75, 'TN': 155, 'FN': 75}            0.82   \n",
       "10        0.68    {'TP': 172, 'FP': 76, 'TN': 154, 'FN': 73}            0.71   \n",
       "11        0.72    {'TP': 148, 'FP': 54, 'TN': 176, 'FN': 97}            0.89   \n",
       "12        0.52      {'TP': 245, 'FP': 229, 'TN': 1, 'FN': 0}            0.52   \n",
       "13        0.63    {'TP': 155, 'FP': 90, 'TN': 140, 'FN': 90}            1.00   \n",
       "14        0.69    {'TP': 170, 'FP': 75, 'TN': 155, 'FN': 75}            0.82   \n",
       "15        0.68    {'TP': 172, 'FP': 76, 'TN': 154, 'FN': 73}            0.71   \n",
       "\n",
       "    test accuracy  baseline accuracy  specificity  f1-score  \n",
       "0            0.67               0.52         0.69      0.67  \n",
       "1            0.58               0.52         0.57      0.58  \n",
       "2            0.69               0.52         0.67      0.69  \n",
       "3            0.68               0.52         0.67      0.68  \n",
       "4            0.54               0.52         0.49      0.54  \n",
       "5            0.67               0.52         0.69      0.67  \n",
       "6            0.68               0.52         0.77      0.68  \n",
       "7            0.52               0.52         0.00      0.36  \n",
       "8            0.62               0.52         0.61      0.62  \n",
       "9            0.68               0.52         0.67      0.68  \n",
       "10           0.69               0.52         0.67      0.69  \n",
       "11           0.68               0.52         0.77      0.68  \n",
       "12           0.52               0.52         0.00      0.36  \n",
       "13           0.62               0.52         0.61      0.62  \n",
       "14           0.68               0.52         0.67      0.68  \n",
       "15           0.69               0.52         0.67      0.69  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the function for out two narrowed models\n",
    "\n",
    "# models\n",
    "steps_titles = ['hvec + multi_nb(tuning)','tvec + multi_nb(tuning)']\n",
    "\n",
    "# pipelines\n",
    "steps_list = [ \n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('multi_nb', MultinomialNB())],\n",
    "    [('tv', TfidfVectorizer()),('multi_nb', MultinomialNB())]\n",
    "]\n",
    "\n",
    "# parameters\n",
    "pipe_params = [\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2),(1,3)], 'hv__n_features': [50, 150, 300, 500, 800, 1000]},\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2),(1,3)],'tv__max_features': [20, 30, 50, 70, 100],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3, .35,.4]},\n",
    "]   \n",
    "\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizor + Multinomial Naive Bayes\n",
    "This model has the best metrics and will work the best for our data. While it isn't the higher AUC, it generalizes better and has higher True Positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running our Optimized Model\n",
    "This model is a combination of TF-IDF(\"Term Frequency - Inverse Document\" Frequency) Vectorizer and the Multinomial Naive Bayes. It assigns scores for the top 70 words in our selected feature. TF-IDF will penalize common words, helping the model find specific key words. The model makes a prediction based on a matrix of word scores and gives a probability of falling into a certain classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       sql witch understand people reply immediately op invitation talk privately mean help type response usually lead either disappointment disaster usually work quite differently say pm anytime casual ...\n",
      "1       c irc welcome r depression check post place take moment share going accomplishment want talk standalone post sub violate role model rule welcome tough time prefer make post place share try best ke...\n",
      "2       new killer 69 feeling really depressed lonely lately job full time late night janitor courthouse 10 mile away hometown working ha pretty easy wound feeling super lonely lot considering really get ...\n",
      "3       jazz le crab literally broke cry asked go home another nurse wa watching wa cry saying wanted keep longer kept saying wanted go home said wa wasting time making watch asked cry leaving never anyth...\n",
      "4                                                     depressed kid 786 kind soul want give depressed person karma edit thanks everyone especially sub hope overcome depression day take easy everyone cake day\n",
      "                                                                                                         ...                                                                                                   \n",
      "1892    big pete 543 hard get bed every morning right want die damn hard alive world want keep going want keep fighting wish would end could peace ever asked life wa love ever wanted wa love world took re...\n",
      "1893    eli sbt low iq twice tested time got iq around 84 85 somehow entirely awful school wa able b student without much studying actually started studying started get grade 90 even 100 even though skimm...\n",
      "1894                                                                                                                                                                 gay h ovum feel useless im useless useless\n",
      "1895    ev e wish wa way could non existence regret entire life know take last breath matter hand regret life stopping know would probably kill dad want daughter feel like would better without would want ...\n",
      "1896                                                                     pon k ichi want die think want feel pain want feel something death way get sorry posting much today anyways im torn rn dont know right\n",
      "Name: megatext_clean, Length: 1897, dtype: object\n",
      "Accuracy: 0.6989473684210527\n",
      "AUC Score: 0.7548003549245785\n"
     ]
    }
   ],
   "source": [
    "# Checking scores of optimized model using test data\n",
    "\n",
    "X = model_data[\"megatext_clean\"]\n",
    "print(X)\n",
    "y = model_data['is_suicide']\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "tvec_optimised = TfidfVectorizer(max_df= 0.5, max_features=70, min_df=2, ngram_range=(1, 3),stop_words = 'english')\n",
    "X_train_tvec = tvec_optimised.fit_transform(X_train).todense()\n",
    "X_test_tvec = tvec_optimised.transform(X_test).todense()\n",
    "\n",
    "# print(X_train_tvec)\n",
    "\n",
    "#getting accuracies of the model\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tvec, y_train)\n",
    "accuracy = nb.score(X_test_tvec, y_test)\n",
    "\n",
    "# calculating AUC\n",
    "\n",
    "pred_proba = [i[1] for i in nb.predict_proba(X_test_tvec)] \n",
    "auc = roc_auc_score(y_test, pred_proba)\n",
    "\n",
    "# save model\n",
    "# pickle.dump(nb, open(\"model.h5\", 'wb'))\n",
    "joblib.dump(nb, \"model2.h5\")\n",
    "\n",
    "print(\"Accuracy: {}\\nAUC Score: {}\".format(accuracy, auc) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performace\n",
    "Our Model performed well, having an accuracy of 70% and an AUC score of 75%. There are most likely ways to improve the classifier, and we will continue to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Most Common Words\n",
    "def TF_IDF_most_used_words(category_string, data_series, palette, image_mask):\n",
    "    # find most common words\n",
    "    tvec_optimised = TfidfVectorizer(max_df= 0.5, max_features=70, min_df=2, ngram_range=(1, 3),stop_words = 'english')\n",
    "    tvec_optimised.fit(data_series)\n",
    "    \n",
    "    # dataframe of extracted words\n",
    "    created_df = pd.DataFrame(tvec_optimised.transform(data_series).todense(),\n",
    "                              columns=tvec_optimised.get_feature_names())\n",
    "    total_words = created_df.sum(axis=0)\n",
    "    \n",
    "    # Top 20 words dataframe\n",
    "    top_20_words = total_words.sort_values(ascending = False).head(20)\n",
    "    top_20_words_df = pd.DataFrame(top_20_words, columns = [\"count\"])\n",
    "    # plotting\n",
    "    sns.set_style(\"white\")\n",
    "    plt.figure(figsize = (15, 8), dpi=300)\n",
    "    ax = sns.barplot(y= top_20_words_df.index, x=\"count\", data=top_20_words_df, palette = palette)\n",
    "    \n",
    "    plt.xlabel(\"Count\", fontsize=9)\n",
    "    plt.ylabel('Common Words in {}'.format(category_string), fontsize=9)\n",
    "    plt.yticks(rotation=-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting X_features\n",
    "TF_IDF_most_used_words(\"Words used by production model to identify r/SuicideWatch Posts\", model_data[\"megatext_clean\"], \"vlag_r\", image_mask=\"../assets/ending_mask_8.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding False Negatives\n",
    "\n",
    "# creating dataframe\n",
    "index_seek = list(X_test.index)\n",
    "fn_explore = model_data.iloc[index_seek, :][[\"title\",\"selftext\",\"author\", \"is_suicide\"]]\n",
    "fn_explore[\"predictions\"] = nb.predict(X_test_tvec)\n",
    "\n",
    "# saving False Negatives\n",
    "#is_suicide == 1 and predictions == 0\n",
    "false_negs = fn_explore[fn_explore[\"is_suicide\"]==1][fn_explore[\"predictions\"]==0]\n",
    "false_negs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing words for False Negatives\n",
    "def barplot_most_used_words(category_string, data_series, palette):\n",
    "    # finding most common words\n",
    "    tvec_optimised = TfidfVectorizer(max_df= 0.5, max_features=70, min_df=2, ngram_range=(1, 3),stop_words = 'english')\n",
    "    tvec_optimised.fit(data_series)\n",
    "    # dataframe \n",
    "    created_df = pd.DataFrame(tvec_optimised.transform(data_series).todense(),\n",
    "                              columns=tvec_optimised.get_feature_names())\n",
    "    total_words = created_df.sum(axis=0)\n",
    "    \n",
    "    top_20_words = total_words.sort_values(ascending = False).head(20)\n",
    "    top_20_words_df = pd.DataFrame(top_20_words, columns = [\"count\"])\n",
    "\n",
    "    # plotting\n",
    "    sns.set_style(\"white\")\n",
    "    plt.figure(figsize = (15, 8), dpi=300)\n",
    "    ax = sns.barplot(y= top_20_words_df.index, x=\"count\", data=top_20_words_df, palette = palette)\n",
    "    plt.title('\\nTop Words used in {}\\n'.format(category_string), fontsize=22)\n",
    "    plt.xlabel(\"Count\", fontsize=9)\n",
    "    plt.ylabel('Common Words in {}'.format(category_string), fontsize=9)\n",
    "    plt.yticks(rotation=-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot_most_used_words(\"False Negatives\", false_negs[\"selftext\"], \"gray\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
