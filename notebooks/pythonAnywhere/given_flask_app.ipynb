{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, redirect, render_template, request, url_for\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "\n",
    "\n",
    "#import torch\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import joblib\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "tf.gfile = tf.io.gfile\n",
    "import bert\n",
    "\n",
    "#from bert import bert_tokenization\n",
    "from bert.tokenization import FullTokenizer\n",
    "\n",
    "from tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow\n",
    "import math\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "\n",
    "#intro flask stuff\n",
    "app = Flask(__name__)\n",
    "app.config[\"DEBUG\"] = True\n",
    "\n",
    "#dictionary for first model\n",
    "thisdict = {\n",
    "  \"brand\": 0,\n",
    "  \"model\": 1,\n",
    "  \"year\": 0\n",
    "}\n",
    "\n",
    "#dictionary for second model\n",
    "progressionDict = {\n",
    "  \"brand\": 0,\n",
    "  \"model\": 1,\n",
    "  \"year\": 0\n",
    "}\n",
    "\n",
    "#rendering the intro html page for first model\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def upload():\n",
    "    return render_template(\"userForm.html\")\n",
    "\n",
    "#rendering the intro html page for second model\n",
    "@app.route(\"/progression\", methods=[\"GET\", \"POST\"])\n",
    "def uploadProgression():\n",
    "    return render_template(\"progressionUserForm.html\")\n",
    "\n",
    "\n",
    "#second model -- takes code from form, puts it through ml, and outputs prediction\n",
    "@app.route(\"/progressionsuccess\",methods=[\"POST\"])\n",
    "def progSuccess():\n",
    "    global stProg\n",
    "    s = request.form['progTextField']\n",
    "    final_model = keras.models.load_model('/home/suiSense/my_site/final_regular_model.h5')\n",
    "\n",
    "    max_seq_length = 128  # Your choice here.\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_mask\")\n",
    "    segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "    bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=True)\n",
    "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])\n",
    "\n",
    "    # See BERT paper: https://arxiv.org/pdf/1810.04805.pdf\n",
    "    # And BERT implementation convert_single_example() at https://github.com/google-research/bert/blob/master/run_classifier.py\n",
    "\n",
    "\n",
    "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "\n",
    "    tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "    stokens = tokenizer.tokenize(s)\n",
    "\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "\n",
    "    input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
    "    input_masks = get_masks(stokens, max_seq_length)\n",
    "    input_segments = get_segments(stokens, max_seq_length)\n",
    "\n",
    "    pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
    "\n",
    "\n",
    "    predictions = final_model.predict(pool_embs)\n",
    "\n",
    "    if (0.0 < predictions <= 0.166):\n",
    "        return render_template(\"progressionSuccess.html\",contents=\"Stage 1: Falling short of expectations\")\n",
    "\n",
    "    elif (0.166 < predictions <= 0.333):\n",
    "        return render_template(\"progressionSuccess.html\",contents=\"Stage 2: Attributions to self\")\n",
    "\n",
    "    elif (0.333 < predictions <= 0.500):\n",
    "        return render_template(\"progressionSuccess.html\",contents=\"Stage 3: High Self-Awareness\")\n",
    "\n",
    "    elif (0.500 < predictions <= 0.667):\n",
    "        return render_template(\"progressionSuccess.html\",contents=\"Stage 4: Negative Affect\")\n",
    "\n",
    "    elif (0.667 < predictions <= 0.833):\n",
    "        return render_template(\"progressionSuccess.html\",contents=\"Stage 5: Cognitive Deconstruction\")\n",
    "\n",
    "    elif (0.833 < predictions <= 1.000):\n",
    "        return render_template(\"progressionSuccess.html\",contents=\"Stage 6: Disinhibition\")\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/success\",methods=[\"POST\"])\n",
    "def success():\n",
    "        h5file =  \"/home/suiSense/my_site/final.h5\"\n",
    "        realSuicidal = \"According to our algorithm, the text has been classified as suicidal.\"\n",
    "        realDepression = \"According to our algorithm, the text has been classified as depression, not suicidal.\"\n",
    "\n",
    "        stOne=request.form['contents']\n",
    "        stTwo = stOne.lower()\n",
    "        stStripped = stTwo.strip()\n",
    "        st = stStripped.replace(\" \", \"\")\n",
    "        prediction = randint(0, 1)\n",
    "\n",
    "        model = joblib.load(h5file)\n",
    "        try:\n",
    "            text_array = pd.Series(request.form['contents'])\n",
    "            processed_text = processing_text(text_array)\n",
    "            processed_array = pd.Series(processed_text)\n",
    "            tvec_optimised = TfidfVectorizer(max_features=70, ngram_range=(1, 3),stop_words = 'english')\n",
    "            processed_text_tvec = tvec_optimised.fit_transform(processed_array).todense()\n",
    "            prediction = model.predict(processed_text_tvec)\n",
    "            Class = prediction[0]\n",
    "\n",
    "            if (Class == 1):\n",
    "                return render_template(\"success.html\",contents=realSuicidal)\n",
    "            else:\n",
    "                return render_template(\"success.html\",contents=realDepression)\n",
    "        except:\n",
    "            return render_template(\"success.html\",contents= \"For a proper result, we need a sequence of atleast 70 words. Your phrase was most likely less. Try to get more phrases put together to get an accurate result.\")\n",
    "\n",
    "\n",
    "\n",
    "def prediction(model, text):\n",
    "    text_array = pd.Series(text)\n",
    "    processed_text = processing_text(text_array)\n",
    "    processed_array = pd.Series(processed_text)\n",
    "    tvec_optimised = TfidfVectorizer(max_features=70, ngram_range=(1, 3),stop_words = 'english')\n",
    "    processed_text_tvec = tvec_optimised.fit_transform(processed_array).todense()\n",
    "    prediction = model.predict(processed_text_tvec)\n",
    "    return(prediction[0])\n",
    "def processing_text(series_to_process):\n",
    "    new_list = []\n",
    "    tokenizer = RegexpTokenizer(r'(\\w+)')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for i in range(len(series_to_process)):\n",
    "        # tokenized item in a new list\n",
    "        dirty_string = (series_to_process)[i].lower()\n",
    "        words_only = tokenizer.tokenize(dirty_string) # words_only is a list of only the words, no punctuation\n",
    "        #Lemmatize the words_only\n",
    "        words_only_lem = [lemmatizer.lemmatize(i) for i in words_only]\n",
    "        # removing stop words\n",
    "        words_without_stop = [i for i in words_only_lem if i not in stopwords.words(\"english\")]\n",
    "        # return seperated words\n",
    "        long_string_clean = \" \".join(word for word in words_without_stop)\n",
    "        new_list.append(long_string_clean)\n",
    "        return new_list\n",
    "\n",
    "def get_masks(tokens, max_seq_length):\n",
    "        \"\"\"Mask for padding\"\"\"\n",
    "        if len(tokens)>max_seq_length:\n",
    "            raise IndexError(\"Token length more than max seq length!\")\n",
    "        return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
