{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCfaTGO3CwGL"
   },
   "source": [
    "## Our Model\n",
    "Here we will be creating different models to classify our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "QQF9iwgnCwGN",
    "outputId": "6b3b6265-3000-465f-c741-8f64a047adcf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set configurations\n",
    "pd.set_option('display.max_columns', 100)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# keras imports\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# model imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpxIwRLmCwGb"
   },
   "outputs": [],
   "source": [
    "model_data = pd.read_csv('https://raw.githubusercontent.com/ayaanzhaque/SuiSense/master/data/data_for_model.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "colab_type": "code",
    "id": "O50SrCuCCwGf",
    "outputId": "5889f455-04d3-49c8-b308-c51dd5ec66e0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_suicide</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>selftext_length</th>\n",
       "      <th>title_length</th>\n",
       "      <th>megatext_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our most-broken and least-understood rules is ...</td>\n",
       "      <td>We understand that most people who reply immed...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/d...</td>\n",
       "      <td>understand people reply immediately op invitat...</td>\n",
       "      <td>broken least understood rule helper may invite...</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>4792</td>\n",
       "      <td>144</td>\n",
       "      <td>sql witch understand people reply immediately ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Regular Check-In Post</td>\n",
       "      <td>Welcome to /r/depression's check-in post - a p...</td>\n",
       "      <td>circinia</td>\n",
       "      <td>1644</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/e...</td>\n",
       "      <td>welcome r depression check post place take mom...</td>\n",
       "      <td>regular check post</td>\n",
       "      <td>c irc</td>\n",
       "      <td>650</td>\n",
       "      <td>21</td>\n",
       "      <td>c irc welcome r depression check post place ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I hate it so much when you try and express you...</td>\n",
       "      <td>I've been feeling really depressed and lonely ...</td>\n",
       "      <td>TheNewKiller69</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/f...</td>\n",
       "      <td>feeling really depressed lonely lately job ful...</td>\n",
       "      <td>hate much try express feeling parent turn arou...</td>\n",
       "      <td>new killer 69</td>\n",
       "      <td>1866</td>\n",
       "      <td>137</td>\n",
       "      <td>new killer 69 feeling really depressed lonely ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Our most-broken and least-understood rules is ...   \n",
       "1                              Regular Check-In Post   \n",
       "2  I hate it so much when you try and express you...   \n",
       "\n",
       "                                            selftext          author  \\\n",
       "0  We understand that most people who reply immed...        SQLwitch   \n",
       "1  Welcome to /r/depression's check-in post - a p...        circinia   \n",
       "2  I've been feeling really depressed and lonely ...  TheNewKiller69   \n",
       "\n",
       "   num_comments  is_suicide  \\\n",
       "0           133           0   \n",
       "1          1644           0   \n",
       "2             8           0   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.reddit.com/r/depression/comments/d...   \n",
       "1  https://www.reddit.com/r/depression/comments/e...   \n",
       "2  https://www.reddit.com/r/depression/comments/f...   \n",
       "\n",
       "                                      selftext_clean  \\\n",
       "0  understand people reply immediately op invitat...   \n",
       "1  welcome r depression check post place take mom...   \n",
       "2  feeling really depressed lonely lately job ful...   \n",
       "\n",
       "                                         title_clean   author_clean  \\\n",
       "0  broken least understood rule helper may invite...      sql witch   \n",
       "1                                 regular check post          c irc   \n",
       "2  hate much try express feeling parent turn arou...  new killer 69   \n",
       "\n",
       "   selftext_length  title_length  \\\n",
       "0             4792           144   \n",
       "1              650            21   \n",
       "2             1866           137   \n",
       "\n",
       "                                      megatext_clean  \n",
       "0  sql witch understand people reply immediately ...  \n",
       "1  c irc welcome r depression check post place ta...  \n",
       "2  new killer 69 feeling really depressed lonely ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "GJA-54VWCwGk",
    "outputId": "f08f2be6-8d4e-4c26-ad00-dab8d04745a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1897 entries, 0 to 1896\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   title            1897 non-null   object\n",
      " 1   selftext         1897 non-null   object\n",
      " 2   author           1897 non-null   object\n",
      " 3   num_comments     1897 non-null   int64 \n",
      " 4   is_suicide       1897 non-null   int64 \n",
      " 5   url              1897 non-null   object\n",
      " 6   selftext_clean   1897 non-null   object\n",
      " 7   title_clean      1897 non-null   object\n",
      " 8   author_clean     1897 non-null   object\n",
      " 9   selftext_length  1897 non-null   int64 \n",
      " 10  title_length     1897 non-null   int64 \n",
      " 11  megatext_clean   1897 non-null   object\n",
      "dtypes: int64(4), object(8)\n",
      "memory usage: 178.0+ KB\n"
     ]
    }
   ],
   "source": [
    "model_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rB4_ZoKPCwGq"
   },
   "source": [
    "Establishing a baseline accuracy is important for evaluating the model's progression. If every prediction was 1, let's see what our accuracy would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nWfHHM4VCwGr",
    "outputId": "6eecaa03-d095-44ac-e9de-490e28bdb2ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5166051660516605"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data['is_suicide'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PfgxVAnrCwGw"
   },
   "source": [
    "Our baseline accuracy is about 51.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awnyv7ftCwGx"
   },
   "source": [
    "### Selection of Features and Attributes\n",
    "Here we are attempting to create a model using CountVectorizer and Naive Bayes Model to determine which columns are the best to score and use for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qM_aEmPmCwGx"
   },
   "outputs": [],
   "source": [
    "# Creating a function to score different models from different columsn in the dataset\n",
    "\n",
    "columns_list = ['column_1', \"column_2\", \"column_3\"]\n",
    "model = \"CountVec + MultinomialNB\"\n",
    "df_list=[] \n",
    "\n",
    "def multi_modelling(columns_list, model):\n",
    "    for i in columns_list:\n",
    "        \n",
    "        X = model_data[i]\n",
    "        y = model_data['is_suicide']\n",
    "        \n",
    "        # train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "        \n",
    "        cvec = CountVectorizer()\n",
    "        cvec.fit(X_train)\n",
    "        \n",
    "        # creating dataframes\n",
    "        X_train = pd.DataFrame(cvec.transform(X_train).todense(),\n",
    "                               columns=cvec.get_feature_names())\n",
    "        X_test = pd.DataFrame(cvec.transform(X_test).todense(),\n",
    "                               columns=cvec.get_feature_names())\n",
    "        \n",
    "        nb = MultinomialNB()\n",
    "        nb.fit(X_train,y_train)\n",
    "        \n",
    "        # get predictions from model\n",
    "        pred = nb.predict(X_test)\n",
    "        \n",
    "        # create confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "        \n",
    "        # calculating AUC\n",
    "        nb.predict_proba(X_test)\n",
    "        pred_proba = [i[1] for i in nb.predict_proba(X_test)] \n",
    "        auc = roc_auc_score(y_test, pred_proba)\n",
    "\n",
    "        # classification report\n",
    "        classi_dict = (classification_report(y_test,pred, output_dict=True))\n",
    "\n",
    "        # storing our results\n",
    "        model_results = {}\n",
    "        model_results['series used (X)'] = i\n",
    "        model_results['model'] = model\n",
    "        model_results['AUC Score'] = auc\n",
    "        model_results['precision']= classi_dict['weighted avg']['precision']\n",
    "        model_results['recall (sensitivity)']= classi_dict['weighted avg']['recall']\n",
    "        model_results['confusion matrix']={\"TP\": tp,\"FP\":fp, \"TN\": tn, \"FN\": fn}\n",
    "        model_results['train accuracy'] = nb.score(X_train, y_train)\n",
    "        model_results['test accuracy'] = nb.score(X_test, y_test)\n",
    "        model_results['baseline accuracy']=0.5166\n",
    "        model_results['specificity']= tn/(tn+fp)  \n",
    "        model_results['f1-score']= classi_dict['weighted avg']['f1-score']\n",
    "        #model_results['support']= classi_dict['weighted avg']['support']\n",
    "        model_results\n",
    "        df_list.append(model_results) \n",
    "\n",
    "    pd.set_option(\"display.max_colwidth\", 50)\n",
    "    return (pd.DataFrame(df_list)).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jKlVOkfiCwG5"
   },
   "source": [
    "For our initial model, it will be a binary classifier. Once the user gets their classification, they will go to another model that will give them specific support. \n",
    "\n",
    "Label Encoding: r/SuicideWatch = 1, r/Depression = 0\n",
    "\n",
    "TP: model predicts suicide, and it is correct\n",
    "\n",
    "TN: model predicts depression, and it is correct\n",
    "\n",
    "FP: model predicts suicide, but it is really depression, not good\n",
    "\n",
    "FN: model predicts depression, but they really are suicidal, this is the worst, misses an at risk patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "JmxyjjuICwG6",
    "outputId": "840793c4-70ef-4b6e-86e3-86ca18dc8c33"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series used (X)</th>\n",
       "      <th>model</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall (sensitivity)</th>\n",
       "      <th>confusion matrix</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>baseline accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>selftext</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>{'TP': 161, 'FP': 78, 'TN': 152, 'FN': 84}</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>author</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.55</td>\n",
       "      <td>{'TP': 235, 'FP': 204, 'TN': 26, 'FN': 10}</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>title</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'TP': 167, 'FP': 104, 'TN': 126, 'FN': 78}</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>selftext_clean</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'TP': 165, 'FP': 78, 'TN': 152, 'FN': 80}</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>author_clean</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.51</td>\n",
       "      <td>{'TP': 169, 'FP': 155, 'TN': 75, 'FN': 76}</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>title_clean</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'TP': 178, 'FP': 112, 'TN': 118, 'FN': 67}</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>megatext_clean</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  series used (X)                     model  AUC Score  precision  \\\n",
       "0        selftext  CountVec + MultinomialNB       0.69       0.66   \n",
       "1          author  CountVec + MultinomialNB       0.57       0.63   \n",
       "2           title  CountVec + MultinomialNB       0.67       0.62   \n",
       "3  selftext_clean  CountVec + MultinomialNB       0.69       0.67   \n",
       "4    author_clean  CountVec + MultinomialNB       0.54       0.51   \n",
       "5     title_clean  CountVec + MultinomialNB       0.67       0.63   \n",
       "6  megatext_clean  CountVec + MultinomialNB       0.71       0.67   \n",
       "\n",
       "   recall (sensitivity)                             confusion matrix  \\\n",
       "0                  0.66   {'TP': 161, 'FP': 78, 'TN': 152, 'FN': 84}   \n",
       "1                  0.55   {'TP': 235, 'FP': 204, 'TN': 26, 'FN': 10}   \n",
       "2                  0.62  {'TP': 167, 'FP': 104, 'TN': 126, 'FN': 78}   \n",
       "3                  0.67   {'TP': 165, 'FP': 78, 'TN': 152, 'FN': 80}   \n",
       "4                  0.51   {'TP': 169, 'FP': 155, 'TN': 75, 'FN': 76}   \n",
       "5                  0.62  {'TP': 178, 'FP': 112, 'TN': 118, 'FN': 67}   \n",
       "6                  0.67   {'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}   \n",
       "\n",
       "   train accuracy  test accuracy  baseline accuracy  specificity  f1-score  \n",
       "0            0.92           0.66               0.52         0.66      0.66  \n",
       "1            0.99           0.55               0.52         0.11      0.45  \n",
       "2            0.85           0.62               0.52         0.55      0.62  \n",
       "3            0.91           0.67               0.52         0.66      0.67  \n",
       "4            0.95           0.51               0.52         0.33      0.50  \n",
       "5            0.84           0.62               0.52         0.51      0.62  \n",
       "6            0.95           0.67               0.52         0.69      0.67  "
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the function\n",
    "columns_list = ['selftext', \"author\", \"title\",'selftext_clean', \"author_clean\", \"title_clean\", \"megatext_clean\"]\n",
    "model = \"CountVec + MultinomialNB\"\n",
    "df_list=[]\n",
    "multi_modelling(columns_list, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RYGi-Jq5CwG_"
   },
   "source": [
    "Based on our results, using the megatext_clean column will be the best option for our model due its good metrics, such as its AUC value. The false negatives are the highest in our confusion matrix, meaning it is good for the model to learn from. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jAU0IhOdCwHB"
   },
   "source": [
    "### Production Model\n",
    "We will test a few different models by running some perumatations and recording the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvR0cpEACwHC"
   },
   "outputs": [],
   "source": [
    "# Defining a function that test different models and record parameters\n",
    "\n",
    "def gridsearch_multi(steps_titles, steps_list, pipe_params):\n",
    "    \n",
    "    X = model_data[\"megatext_clean\"]\n",
    "    y = model_data['is_suicide']\n",
    "    \n",
    "    # train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "    # build dataframe\n",
    "    gs_results = pd.DataFrame(columns=['model','AUC Score', 'precision', 'recall (sensitivity)', \n",
    "                                       'best_params', 'best score', 'confusion matrix', \n",
    "                                       'train_accuracy','test_accuracy','baseline_accuracy',\n",
    "                                       'specificity', 'f1-score'])\n",
    "\n",
    "    for i in range(len(steps_list)):\n",
    "        # define pipeline\n",
    "        pipe = Pipeline(steps=steps_list[i])\n",
    "\n",
    "        gs = GridSearchCV(pipe, pipe_params[i], cv=3) \n",
    "        gs.fit(X_train, y_train)\n",
    "        \n",
    "        # getting predictions from model\n",
    "        pred = gs.predict(X_test)\n",
    "        \n",
    "        # confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, gs.predict(X_test)).ravel() \n",
    "        \n",
    "        classi_dict = (classification_report(y_test,pred, output_dict=True))\n",
    "        \n",
    "        # calculating AUC\n",
    "        gs.predict_proba(X_test)\n",
    "        pred_proba = [i[1] for i in gs.predict_proba(X_test)] \n",
    "        auc = roc_auc_score(y_test, pred_proba)\n",
    "        \n",
    "        # create dataframe columns\n",
    "        model_results = {}\n",
    "        model_results['model'] = steps_titles[i]\n",
    "        model_results['AUC Score'] = auc\n",
    "        model_results['precision']= classi_dict['weighted avg']['precision']\n",
    "        model_results['recall (sensitivity)']= classi_dict['weighted avg']['recall']\n",
    "        model_results['best params'] = gs.best_params_\n",
    "        model_results['best score'] = gs.best_score_\n",
    "        model_results['confusion matrix']={\"TP\": tp,\"FP\":fp, \"TN\": tn, \"FN\": fn}\n",
    "        model_results['train accuracy'] = gs.score(X_train, y_train)\n",
    "        model_results['test accuracy'] = gs.score(X_test, y_test)\n",
    "        model_results['baseline accuracy'] = 0.5166\n",
    "        \n",
    "        model_results['specificity']= tn/(tn+fp)  \n",
    "        model_results['f1-score']= classi_dict['weighted avg']['f1-score']\n",
    "\n",
    "\n",
    "        # add data\n",
    "        df_list.append(model_results) \n",
    "        pd.set_option(\"display.max_colwidth\", 200)\n",
    "    return (pd.DataFrame(df_list)).round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfc_xnCYCwHG",
    "outputId": "48ebf95b-6b66-4d67-e5b2-8d011cfc2a30"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3dd3525bc634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m ]   \n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mgridsearch_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_titles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-77b72ff60de0>\u001b[0m in \u001b[0;36mgridsearch_multi\u001b[0;34m(steps_titles, steps_list, pipe_params)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# getting predictions from model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[1;32m    329\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    332\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    293\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0mindices_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0mj_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m         \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# testing function with count vectorizor\n",
    "\n",
    "df_list=[]\n",
    "\n",
    "# models\n",
    "steps_titles = ['cvec+ multi_nb','cvec + ss + knn','cvec + ss + logreg']\n",
    "\n",
    "# pipelines\n",
    "steps_list = [ \n",
    "    [('cv', CountVectorizer()),('multi_nb', MultinomialNB())],\n",
    "    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]\n",
    "\n",
    "# parameters\n",
    "pipe_params = [\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]},\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]},\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]}\n",
    "]   \n",
    "\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9m0NInVCCwHL"
   },
   "outputs": [],
   "source": [
    "# testing function with TFD Vectorizor\n",
    "\n",
    "# models\n",
    "steps_titles = ['tvec + multi_nb','tvec + ss + knn','tvec + ss + logreg']\n",
    "\n",
    "# pipelines\n",
    "steps_list = [ \n",
    "    [('tv', TfidfVectorizer()),('multi_nb', MultinomialNB())],\n",
    "    [('tv', TfidfVectorizer()),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('tv', TfidfVectorizer()),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]\n",
    "\n",
    "# parameters\n",
    "pipe_params = [\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]},\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]},\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]}\n",
    "]   \n",
    "\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vpqaKLeMCwHP"
   },
   "outputs": [],
   "source": [
    "# testing function with Hashing Vectorizor\n",
    "\n",
    "# models\n",
    "steps_titles = ['hvec + multi_nb','hvec + ss + knn','hvec + ss + logreg']\n",
    "\n",
    "# pipelines\n",
    "steps_list = [ \n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('multi_nb', MultinomialNB())],\n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]\n",
    "\n",
    "# parameters\n",
    "pipe_params = [\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]},\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]},\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]}\n",
    "]   \n",
    "\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WlhF5QNDCwHS"
   },
   "source": [
    "After reviewing all the performances, two models stand out. The Hashing Vectorizor and Multinomial Naive Bayes model had much better metrics, along with the TFID Vectorizor and Multinomial Naive Bayes Model. They both have good AUC values and high generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XIyckvRyCwHU"
   },
   "source": [
    "### Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibkImEdmCwHV"
   },
   "outputs": [],
   "source": [
    "# using the function for out two narrowed models\n",
    "\n",
    "# models\n",
    "steps_titles = ['hvec + multi_nb(tuning)','tvec + multi_nb(tuning)']\n",
    "\n",
    "# pipelines\n",
    "steps_list = [ \n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('multi_nb', MultinomialNB())],\n",
    "    [('tv', TfidfVectorizer()),('multi_nb', MultinomialNB())]\n",
    "]\n",
    "\n",
    "# parameters\n",
    "pipe_params = [\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2),(1,3)], 'hv__n_features': [50, 150, 300, 500, 800, 1000]},\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2),(1,3)],'tv__max_features': [20, 30, 50, 70, 100],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3, .35,.4]},\n",
    "]   \n",
    "\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6iILFOuKCwHa"
   },
   "source": [
    "### TF-IDF Vectorizor + Multinomial Naive Bayes\n",
    "This model has the best metrics and will work the best for our data. While it isn't the higher AUC, it generalizes better and has higher True Positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "20Dxs2deCwHb"
   },
   "source": [
    "## Running our Optimized Model\n",
    "This model is a combination of TF-IDF(\"Term Frequency - Inverse Document\" Frequency) Vectorizer and the Multinomial Naive Bayes. It assigns scores for the top 70 words in our selected feature. TF-IDF will penalize common words, helping the model find specific key words. The model makes a prediction based on a matrix of word scores and gives a probability of falling into a certain classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qj2GpgECwHh"
   },
   "outputs": [],
   "source": [
    "# getting ready for training\n",
    "\n",
    "X = model_data[\"selftext_clean\"]\n",
    "y = model_data['is_suicide']\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "tvec_optimised = TfidfVectorizer(max_df= 0.5, max_features=70, min_df=2, ngram_range=(1, 3),stop_words = 'english')\n",
    "X_train_tvec = tvec_optimised.fit_transform(X_train).todense()\n",
    "X_test_tvec = tvec_optimised.transform(X_test).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5j_YXnVECwHk"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=70)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8jPFS1GkCwHn"
   },
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 70))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFKhrNzpCwHr"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, 70, weights=[embedding_matrix], input_length=70 , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(keras.layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ywdh_uNUEugV"
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(10, input_dim=70, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vpyU1cB-HDzo"
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(10, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oT1Te39FIDoL"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.add(Dense(10, activation='relu'))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Z_SFCtn3CwHw",
    "outputId": "d113b3e6-cfb2-489e-e9b8-69815dfd6216"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3471 - accuracy: 0.8550 - val_loss: 0.7926 - val_accuracy: 0.6395\n",
      "Epoch 2/100\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3469 - accuracy: 0.8563 - val_loss: 0.7919 - val_accuracy: 0.6421\n",
      "Epoch 3/100\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3457 - accuracy: 0.8609 - val_loss: 0.7931 - val_accuracy: 0.6368\n",
      "Epoch 4/100\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3450 - accuracy: 0.8616 - val_loss: 0.7924 - val_accuracy: 0.6395\n",
      "Epoch 5/100\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3445 - accuracy: 0.8629 - val_loss: 0.7946 - val_accuracy: 0.6421\n",
      "Epoch 6/100\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3448 - accuracy: 0.8616 - val_loss: 0.7966 - val_accuracy: 0.6395\n",
      "Epoch 7/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3442 - accuracy: 0.8576 - val_loss: 0.7975 - val_accuracy: 0.6395\n",
      "Epoch 8/100\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3425 - accuracy: 0.8589 - val_loss: 0.7982 - val_accuracy: 0.6447\n",
      "Epoch 9/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3429 - accuracy: 0.8635 - val_loss: 0.8008 - val_accuracy: 0.6395\n",
      "Epoch 10/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3420 - accuracy: 0.8622 - val_loss: 0.8010 - val_accuracy: 0.6368\n",
      "Epoch 11/100\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3411 - accuracy: 0.8609 - val_loss: 0.8017 - val_accuracy: 0.6368\n",
      "Epoch 12/100\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3412 - accuracy: 0.8596 - val_loss: 0.8028 - val_accuracy: 0.6395\n",
      "Epoch 13/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3404 - accuracy: 0.8609 - val_loss: 0.8033 - val_accuracy: 0.6395\n",
      "Epoch 14/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3398 - accuracy: 0.8622 - val_loss: 0.8032 - val_accuracy: 0.6395\n",
      "Epoch 15/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3393 - accuracy: 0.8629 - val_loss: 0.8062 - val_accuracy: 0.6395\n",
      "Epoch 16/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3392 - accuracy: 0.8629 - val_loss: 0.8080 - val_accuracy: 0.6395\n",
      "Epoch 17/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3384 - accuracy: 0.8609 - val_loss: 0.8077 - val_accuracy: 0.6368\n",
      "Epoch 18/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3381 - accuracy: 0.8635 - val_loss: 0.8093 - val_accuracy: 0.6368\n",
      "Epoch 19/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3372 - accuracy: 0.8629 - val_loss: 0.8099 - val_accuracy: 0.6368\n",
      "Epoch 20/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.8642 - val_loss: 0.8113 - val_accuracy: 0.6342\n",
      "Epoch 21/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3360 - accuracy: 0.8616 - val_loss: 0.8121 - val_accuracy: 0.6395\n",
      "Epoch 22/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3357 - accuracy: 0.8635 - val_loss: 0.8137 - val_accuracy: 0.6395\n",
      "Epoch 23/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8682 - val_loss: 0.8134 - val_accuracy: 0.6395\n",
      "Epoch 24/100\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3349 - accuracy: 0.8622 - val_loss: 0.8135 - val_accuracy: 0.6474\n",
      "Epoch 25/100\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3346 - accuracy: 0.8662 - val_loss: 0.8145 - val_accuracy: 0.6368\n",
      "Epoch 26/100\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3338 - accuracy: 0.8649 - val_loss: 0.8168 - val_accuracy: 0.6421\n",
      "Epoch 27/100\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3331 - accuracy: 0.8662 - val_loss: 0.8185 - val_accuracy: 0.6395\n",
      "Epoch 28/100\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3323 - accuracy: 0.8668 - val_loss: 0.8179 - val_accuracy: 0.6395\n",
      "Epoch 29/100\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3320 - accuracy: 0.8655 - val_loss: 0.8210 - val_accuracy: 0.6474\n",
      "Epoch 30/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.8649 - val_loss: 0.8207 - val_accuracy: 0.6395\n",
      "Epoch 31/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3314 - accuracy: 0.8635 - val_loss: 0.8207 - val_accuracy: 0.6395\n",
      "Epoch 32/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3305 - accuracy: 0.8682 - val_loss: 0.8232 - val_accuracy: 0.6447\n",
      "Epoch 33/100\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3297 - accuracy: 0.8701 - val_loss: 0.8235 - val_accuracy: 0.6395\n",
      "Epoch 34/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3290 - accuracy: 0.8635 - val_loss: 0.8238 - val_accuracy: 0.6500\n",
      "Epoch 35/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3299 - accuracy: 0.8701 - val_loss: 0.8242 - val_accuracy: 0.6447\n",
      "Epoch 36/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3283 - accuracy: 0.8675 - val_loss: 0.8278 - val_accuracy: 0.6474\n",
      "Epoch 37/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3291 - accuracy: 0.8675 - val_loss: 0.8278 - val_accuracy: 0.6421\n",
      "Epoch 38/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3277 - accuracy: 0.8715 - val_loss: 0.8275 - val_accuracy: 0.6474\n",
      "Epoch 39/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3270 - accuracy: 0.8721 - val_loss: 0.8293 - val_accuracy: 0.6421\n",
      "Epoch 40/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3267 - accuracy: 0.8708 - val_loss: 0.8295 - val_accuracy: 0.6447\n",
      "Epoch 41/100\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3260 - accuracy: 0.8748 - val_loss: 0.8303 - val_accuracy: 0.6421\n",
      "Epoch 42/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3248 - accuracy: 0.8715 - val_loss: 0.8332 - val_accuracy: 0.6474\n",
      "Epoch 43/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3248 - accuracy: 0.8748 - val_loss: 0.8319 - val_accuracy: 0.6500\n",
      "Epoch 44/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3243 - accuracy: 0.8688 - val_loss: 0.8329 - val_accuracy: 0.6395\n",
      "Epoch 45/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3238 - accuracy: 0.8728 - val_loss: 0.8338 - val_accuracy: 0.6395\n",
      "Epoch 46/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3239 - accuracy: 0.8688 - val_loss: 0.8354 - val_accuracy: 0.6500\n",
      "Epoch 47/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.8715 - val_loss: 0.8358 - val_accuracy: 0.6500\n",
      "Epoch 48/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3239 - accuracy: 0.8662 - val_loss: 0.8374 - val_accuracy: 0.6553\n",
      "Epoch 49/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3239 - accuracy: 0.8701 - val_loss: 0.8381 - val_accuracy: 0.6526\n",
      "Epoch 50/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3210 - accuracy: 0.8787 - val_loss: 0.8409 - val_accuracy: 0.6368\n",
      "Epoch 51/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3221 - accuracy: 0.8708 - val_loss: 0.8397 - val_accuracy: 0.6447\n",
      "Epoch 52/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3207 - accuracy: 0.8728 - val_loss: 0.8421 - val_accuracy: 0.6368\n",
      "Epoch 53/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3203 - accuracy: 0.8748 - val_loss: 0.8424 - val_accuracy: 0.6500\n",
      "Epoch 54/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3198 - accuracy: 0.8767 - val_loss: 0.8414 - val_accuracy: 0.6500\n",
      "Epoch 55/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3190 - accuracy: 0.8787 - val_loss: 0.8450 - val_accuracy: 0.6447\n",
      "Epoch 56/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3185 - accuracy: 0.8734 - val_loss: 0.8450 - val_accuracy: 0.6395\n",
      "Epoch 57/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3192 - accuracy: 0.8780 - val_loss: 0.8447 - val_accuracy: 0.6500\n",
      "Epoch 58/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3180 - accuracy: 0.8767 - val_loss: 0.8466 - val_accuracy: 0.6500\n",
      "Epoch 59/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3175 - accuracy: 0.8754 - val_loss: 0.8478 - val_accuracy: 0.6500\n",
      "Epoch 60/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3158 - accuracy: 0.8800 - val_loss: 0.8483 - val_accuracy: 0.6447\n",
      "Epoch 61/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3164 - accuracy: 0.8774 - val_loss: 0.8495 - val_accuracy: 0.6342\n",
      "Epoch 62/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3167 - accuracy: 0.8774 - val_loss: 0.8511 - val_accuracy: 0.6474\n",
      "Epoch 63/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3154 - accuracy: 0.8800 - val_loss: 0.8496 - val_accuracy: 0.6553\n",
      "Epoch 64/100\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.3144 - accuracy: 0.8820 - val_loss: 0.8536 - val_accuracy: 0.6500\n",
      "Epoch 65/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3137 - accuracy: 0.8794 - val_loss: 0.8531 - val_accuracy: 0.6500\n",
      "Epoch 66/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3141 - accuracy: 0.8794 - val_loss: 0.8555 - val_accuracy: 0.6500\n",
      "Epoch 67/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3149 - accuracy: 0.8767 - val_loss: 0.8545 - val_accuracy: 0.6526\n",
      "Epoch 68/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3127 - accuracy: 0.8820 - val_loss: 0.8581 - val_accuracy: 0.6447\n",
      "Epoch 69/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3124 - accuracy: 0.8833 - val_loss: 0.8569 - val_accuracy: 0.6447\n",
      "Epoch 70/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3120 - accuracy: 0.8853 - val_loss: 0.8598 - val_accuracy: 0.6474\n",
      "Epoch 71/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3116 - accuracy: 0.8840 - val_loss: 0.8595 - val_accuracy: 0.6526\n",
      "Epoch 72/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3112 - accuracy: 0.8873 - val_loss: 0.8625 - val_accuracy: 0.6447\n",
      "Epoch 73/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3110 - accuracy: 0.8860 - val_loss: 0.8610 - val_accuracy: 0.6474\n",
      "Epoch 74/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.8840 - val_loss: 0.8635 - val_accuracy: 0.6526\n",
      "Epoch 75/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3103 - accuracy: 0.8794 - val_loss: 0.8637 - val_accuracy: 0.6553\n",
      "Epoch 76/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3091 - accuracy: 0.8873 - val_loss: 0.8635 - val_accuracy: 0.6526\n",
      "Epoch 77/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3089 - accuracy: 0.8846 - val_loss: 0.8648 - val_accuracy: 0.6500\n",
      "Epoch 78/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3087 - accuracy: 0.8846 - val_loss: 0.8656 - val_accuracy: 0.6421\n",
      "Epoch 79/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3079 - accuracy: 0.8873 - val_loss: 0.8658 - val_accuracy: 0.6553\n",
      "Epoch 80/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3069 - accuracy: 0.8860 - val_loss: 0.8699 - val_accuracy: 0.6395\n",
      "Epoch 81/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3063 - accuracy: 0.8866 - val_loss: 0.8695 - val_accuracy: 0.6474\n",
      "Epoch 82/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3075 - accuracy: 0.8893 - val_loss: 0.8697 - val_accuracy: 0.6500\n",
      "Epoch 83/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3058 - accuracy: 0.8886 - val_loss: 0.8701 - val_accuracy: 0.6447\n",
      "Epoch 84/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3060 - accuracy: 0.8879 - val_loss: 0.8709 - val_accuracy: 0.6553\n",
      "Epoch 85/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3066 - accuracy: 0.8866 - val_loss: 0.8719 - val_accuracy: 0.6526\n",
      "Epoch 86/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3049 - accuracy: 0.8866 - val_loss: 0.8706 - val_accuracy: 0.6579\n",
      "Epoch 87/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3041 - accuracy: 0.8893 - val_loss: 0.8740 - val_accuracy: 0.6553\n",
      "Epoch 88/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3036 - accuracy: 0.8853 - val_loss: 0.8751 - val_accuracy: 0.6500\n",
      "Epoch 89/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3038 - accuracy: 0.8879 - val_loss: 0.8749 - val_accuracy: 0.6526\n",
      "Epoch 90/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3040 - accuracy: 0.8853 - val_loss: 0.8739 - val_accuracy: 0.6579\n",
      "Epoch 91/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3030 - accuracy: 0.8906 - val_loss: 0.8784 - val_accuracy: 0.6500\n",
      "Epoch 92/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3025 - accuracy: 0.8906 - val_loss: 0.8773 - val_accuracy: 0.6553\n",
      "Epoch 93/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3013 - accuracy: 0.8919 - val_loss: 0.8812 - val_accuracy: 0.6447\n",
      "Epoch 94/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3012 - accuracy: 0.8932 - val_loss: 0.8792 - val_accuracy: 0.6553\n",
      "Epoch 95/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3016 - accuracy: 0.8945 - val_loss: 0.8793 - val_accuracy: 0.6474\n",
      "Epoch 96/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3002 - accuracy: 0.8926 - val_loss: 0.8831 - val_accuracy: 0.6526\n",
      "Epoch 97/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.2998 - accuracy: 0.8899 - val_loss: 0.8821 - val_accuracy: 0.6553\n",
      "Epoch 98/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.3000 - accuracy: 0.8886 - val_loss: 0.8841 - val_accuracy: 0.6553\n",
      "Epoch 99/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.2992 - accuracy: 0.8926 - val_loss: 0.8849 - val_accuracy: 0.6579\n",
      "Epoch 100/100\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 0.2986 - accuracy: 0.8866 - val_loss: 0.8880 - val_accuracy: 0.6395\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(X_train_tvec, y_train, batch_size=32, epochs=100, verbose=1, validation_data=(X_test_tvec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1gBZxUFCwH2"
   },
   "source": [
    "### Model Performace\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
