{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCfaTGO3CwGL"
   },
   "source": [
    "## Our Model\n",
    "Here we will be creating different models to classify our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "QQF9iwgnCwGN",
    "outputId": "6b3b6265-3000-465f-c741-8f64a047adcf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set configurations\n",
    "pd.set_option('display.max_columns', 100)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# keras imports\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# model imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpxIwRLmCwGb"
   },
   "outputs": [],
   "source": [
    "model_data = pd.read_csv('https://raw.githubusercontent.com/ayaanzhaque/SuiSense/master/data/data_for_model.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "colab_type": "code",
    "id": "O50SrCuCCwGf",
    "outputId": "5889f455-04d3-49c8-b308-c51dd5ec66e0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_suicide</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>selftext_length</th>\n",
       "      <th>title_length</th>\n",
       "      <th>megatext_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our most-broken and least-understood rules is ...</td>\n",
       "      <td>We understand that most people who reply immed...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/d...</td>\n",
       "      <td>understand people reply immediately op invitat...</td>\n",
       "      <td>broken least understood rule helper may invite...</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>4792</td>\n",
       "      <td>144</td>\n",
       "      <td>sql witch understand people reply immediately ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Regular Check-In Post</td>\n",
       "      <td>Welcome to /r/depression's check-in post - a p...</td>\n",
       "      <td>circinia</td>\n",
       "      <td>1644</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/e...</td>\n",
       "      <td>welcome r depression check post place take mom...</td>\n",
       "      <td>regular check post</td>\n",
       "      <td>c irc</td>\n",
       "      <td>650</td>\n",
       "      <td>21</td>\n",
       "      <td>c irc welcome r depression check post place ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I hate it so much when you try and express you...</td>\n",
       "      <td>I've been feeling really depressed and lonely ...</td>\n",
       "      <td>TheNewKiller69</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/f...</td>\n",
       "      <td>feeling really depressed lonely lately job ful...</td>\n",
       "      <td>hate much try express feeling parent turn arou...</td>\n",
       "      <td>new killer 69</td>\n",
       "      <td>1866</td>\n",
       "      <td>137</td>\n",
       "      <td>new killer 69 feeling really depressed lonely ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Our most-broken and least-understood rules is ...   \n",
       "1                              Regular Check-In Post   \n",
       "2  I hate it so much when you try and express you...   \n",
       "\n",
       "                                            selftext          author  \\\n",
       "0  We understand that most people who reply immed...        SQLwitch   \n",
       "1  Welcome to /r/depression's check-in post - a p...        circinia   \n",
       "2  I've been feeling really depressed and lonely ...  TheNewKiller69   \n",
       "\n",
       "   num_comments  is_suicide  \\\n",
       "0           133           0   \n",
       "1          1644           0   \n",
       "2             8           0   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.reddit.com/r/depression/comments/d...   \n",
       "1  https://www.reddit.com/r/depression/comments/e...   \n",
       "2  https://www.reddit.com/r/depression/comments/f...   \n",
       "\n",
       "                                      selftext_clean  \\\n",
       "0  understand people reply immediately op invitat...   \n",
       "1  welcome r depression check post place take mom...   \n",
       "2  feeling really depressed lonely lately job ful...   \n",
       "\n",
       "                                         title_clean   author_clean  \\\n",
       "0  broken least understood rule helper may invite...      sql witch   \n",
       "1                                 regular check post          c irc   \n",
       "2  hate much try express feeling parent turn arou...  new killer 69   \n",
       "\n",
       "   selftext_length  title_length  \\\n",
       "0             4792           144   \n",
       "1              650            21   \n",
       "2             1866           137   \n",
       "\n",
       "                                      megatext_clean  \n",
       "0  sql witch understand people reply immediately ...  \n",
       "1  c irc welcome r depression check post place ta...  \n",
       "2  new killer 69 feeling really depressed lonely ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "GJA-54VWCwGk",
    "outputId": "f08f2be6-8d4e-4c26-ad00-dab8d04745a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1897 entries, 0 to 1896\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   title            1897 non-null   object\n",
      " 1   selftext         1897 non-null   object\n",
      " 2   author           1897 non-null   object\n",
      " 3   num_comments     1897 non-null   int64 \n",
      " 4   is_suicide       1897 non-null   int64 \n",
      " 5   url              1897 non-null   object\n",
      " 6   selftext_clean   1897 non-null   object\n",
      " 7   title_clean      1897 non-null   object\n",
      " 8   author_clean     1897 non-null   object\n",
      " 9   selftext_length  1897 non-null   int64 \n",
      " 10  title_length     1897 non-null   int64 \n",
      " 11  megatext_clean   1897 non-null   object\n",
      "dtypes: int64(4), object(8)\n",
      "memory usage: 178.0+ KB\n"
     ]
    }
   ],
   "source": [
    "model_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rB4_ZoKPCwGq"
   },
   "source": [
    "Establishing a baseline accuracy is important for evaluating the model's progression. If every prediction was 1, let's see what our accuracy would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nWfHHM4VCwGr",
    "outputId": "6eecaa03-d095-44ac-e9de-490e28bdb2ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5166051660516605"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data['is_suicide'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PfgxVAnrCwGw"
   },
   "source": [
    "Our baseline accuracy is about 51.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awnyv7ftCwGx"
   },
   "source": [
    "### Selection of Features and Attributes\n",
    "Here we are attempting to create a model using CountVectorizer and Naive Bayes Model to determine which columns are the best to score and use for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qM_aEmPmCwGx"
   },
   "outputs": [],
   "source": [
    "# Creating a function to score different models from different columsn in the dataset\n",
    "\n",
    "columns_list = ['column_1', \"column_2\", \"column_3\"]\n",
    "model = \"CountVec + MultinomialNB\"\n",
    "df_list=[] \n",
    "\n",
    "def multi_modelling(columns_list, model):\n",
    "    for i in columns_list:\n",
    "        \n",
    "        X = model_data[i]\n",
    "        y = model_data['is_suicide']\n",
    "        \n",
    "        # train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "        \n",
    "        cvec = CountVectorizer()\n",
    "        cvec.fit(X_train)\n",
    "        \n",
    "        # creating dataframes\n",
    "        X_train = pd.DataFrame(cvec.transform(X_train).todense(),\n",
    "                               columns=cvec.get_feature_names())\n",
    "        X_test = pd.DataFrame(cvec.transform(X_test).todense(),\n",
    "                               columns=cvec.get_feature_names())\n",
    "        \n",
    "        nb = MultinomialNB()\n",
    "        nb.fit(X_train,y_train)\n",
    "        \n",
    "        # get predictions from model\n",
    "        pred = nb.predict(X_test)\n",
    "        \n",
    "        # create confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "        \n",
    "        # calculating AUC\n",
    "        nb.predict_proba(X_test)\n",
    "        pred_proba = [i[1] for i in nb.predict_proba(X_test)] \n",
    "        auc = roc_auc_score(y_test, pred_proba)\n",
    "\n",
    "        # classification report\n",
    "        classi_dict = (classification_report(y_test,pred, output_dict=True))\n",
    "\n",
    "        # storing our results\n",
    "        model_results = {}\n",
    "        model_results['series used (X)'] = i\n",
    "        model_results['model'] = model\n",
    "        model_results['AUC Score'] = auc\n",
    "        model_results['precision']= classi_dict['weighted avg']['precision']\n",
    "        model_results['recall (sensitivity)']= classi_dict['weighted avg']['recall']\n",
    "        model_results['confusion matrix']={\"TP\": tp,\"FP\":fp, \"TN\": tn, \"FN\": fn}\n",
    "        model_results['train accuracy'] = nb.score(X_train, y_train)\n",
    "        model_results['test accuracy'] = nb.score(X_test, y_test)\n",
    "        model_results['baseline accuracy']=0.5166\n",
    "        model_results['specificity']= tn/(tn+fp)  \n",
    "        model_results['f1-score']= classi_dict['weighted avg']['f1-score']\n",
    "        #model_results['support']= classi_dict['weighted avg']['support']\n",
    "        model_results\n",
    "        df_list.append(model_results) \n",
    "\n",
    "    pd.set_option(\"display.max_colwidth\", 50)\n",
    "    return (pd.DataFrame(df_list)).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jKlVOkfiCwG5"
   },
   "source": [
    "For our initial model, it will be a binary classifier. Once the user gets their classification, they will go to another model that will give them specific support. \n",
    "\n",
    "Label Encoding: r/SuicideWatch = 1, r/Depression = 0\n",
    "\n",
    "TP: model predicts suicide, and it is correct\n",
    "\n",
    "TN: model predicts depression, and it is correct\n",
    "\n",
    "FP: model predicts suicide, but it is really depression, not good\n",
    "\n",
    "FN: model predicts depression, but they really are suicidal, this is the worst, misses an at risk patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "JmxyjjuICwG6",
    "outputId": "840793c4-70ef-4b6e-86e3-86ca18dc8c33"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series used (X)</th>\n",
       "      <th>model</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall (sensitivity)</th>\n",
       "      <th>confusion matrix</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>baseline accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>selftext</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>{'TP': 161, 'FP': 78, 'TN': 152, 'FN': 84}</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>author</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.55</td>\n",
       "      <td>{'TP': 235, 'FP': 204, 'TN': 26, 'FN': 10}</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>title</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'TP': 167, 'FP': 104, 'TN': 126, 'FN': 78}</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>selftext_clean</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'TP': 165, 'FP': 78, 'TN': 152, 'FN': 80}</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>author_clean</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.51</td>\n",
       "      <td>{'TP': 169, 'FP': 155, 'TN': 75, 'FN': 76}</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>title_clean</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'TP': 178, 'FP': 112, 'TN': 118, 'FN': 67}</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>megatext_clean</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  series used (X)                     model  AUC Score  precision  \\\n",
       "0        selftext  CountVec + MultinomialNB       0.69       0.66   \n",
       "1          author  CountVec + MultinomialNB       0.57       0.63   \n",
       "2           title  CountVec + MultinomialNB       0.67       0.62   \n",
       "3  selftext_clean  CountVec + MultinomialNB       0.69       0.67   \n",
       "4    author_clean  CountVec + MultinomialNB       0.54       0.51   \n",
       "5     title_clean  CountVec + MultinomialNB       0.67       0.63   \n",
       "6  megatext_clean  CountVec + MultinomialNB       0.71       0.67   \n",
       "\n",
       "   recall (sensitivity)                             confusion matrix  \\\n",
       "0                  0.66   {'TP': 161, 'FP': 78, 'TN': 152, 'FN': 84}   \n",
       "1                  0.55   {'TP': 235, 'FP': 204, 'TN': 26, 'FN': 10}   \n",
       "2                  0.62  {'TP': 167, 'FP': 104, 'TN': 126, 'FN': 78}   \n",
       "3                  0.67   {'TP': 165, 'FP': 78, 'TN': 152, 'FN': 80}   \n",
       "4                  0.51   {'TP': 169, 'FP': 155, 'TN': 75, 'FN': 76}   \n",
       "5                  0.62  {'TP': 178, 'FP': 112, 'TN': 118, 'FN': 67}   \n",
       "6                  0.67   {'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}   \n",
       "\n",
       "   train accuracy  test accuracy  baseline accuracy  specificity  f1-score  \n",
       "0            0.92           0.66               0.52         0.66      0.66  \n",
       "1            0.99           0.55               0.52         0.11      0.45  \n",
       "2            0.85           0.62               0.52         0.55      0.62  \n",
       "3            0.91           0.67               0.52         0.66      0.67  \n",
       "4            0.95           0.51               0.52         0.33      0.50  \n",
       "5            0.84           0.62               0.52         0.51      0.62  \n",
       "6            0.95           0.67               0.52         0.69      0.67  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the function\n",
    "columns_list = ['selftext', \"author\", \"title\",'selftext_clean', \"author_clean\", \"title_clean\", \"megatext_clean\"]\n",
    "model = \"CountVec + MultinomialNB\"\n",
    "df_list=[]\n",
    "multi_modelling(columns_list, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RYGi-Jq5CwG_"
   },
   "source": [
    "Based on our results, using the megatext_clean column will be the best option for our model due its good metrics, such as its AUC value. The false negatives are the highest in our confusion matrix, meaning it is good for the model to learn from. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jAU0IhOdCwHB"
   },
   "source": [
    "### Production Model\n",
    "We will test a few different models by running some perumatations and recording the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvR0cpEACwHC"
   },
   "outputs": [],
   "source": [
    "# Defining a function that test different models and record parameters\n",
    "\n",
    "def gridsearch_multi(steps_titles, steps_list, pipe_params):\n",
    "    \n",
    "    X = model_data[\"megatext_clean\"]\n",
    "    y = model_data['is_suicide']\n",
    "    \n",
    "    # train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "    # build dataframe\n",
    "    gs_results = pd.DataFrame(columns=['model','AUC Score', 'precision', 'recall (sensitivity)', \n",
    "                                       'best_params', 'best score', 'confusion matrix', \n",
    "                                       'train_accuracy','test_accuracy','baseline_accuracy',\n",
    "                                       'specificity', 'f1-score'])\n",
    "\n",
    "    for i in range(len(steps_list)):\n",
    "        # define pipeline\n",
    "        pipe = Pipeline(steps=steps_list[i])\n",
    "\n",
    "        gs = GridSearchCV(pipe, pipe_params[i], cv=3) \n",
    "        gs.fit(X_train, y_train)\n",
    "        \n",
    "        # getting predictions from model\n",
    "        pred = gs.predict(X_test)\n",
    "        \n",
    "        # confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, gs.predict(X_test)).ravel() \n",
    "        \n",
    "        classi_dict = (classification_report(y_test,pred, output_dict=True))\n",
    "        \n",
    "        # calculating AUC\n",
    "        gs.predict_proba(X_test)\n",
    "        pred_proba = [i[1] for i in gs.predict_proba(X_test)] \n",
    "        auc = roc_auc_score(y_test, pred_proba)\n",
    "        \n",
    "        # create dataframe columns\n",
    "        model_results = {}\n",
    "        model_results['model'] = steps_titles[i]\n",
    "        model_results['AUC Score'] = auc\n",
    "        model_results['precision']= classi_dict['weighted avg']['precision']\n",
    "        model_results['recall (sensitivity)']= classi_dict['weighted avg']['recall']\n",
    "        model_results['best params'] = gs.best_params_\n",
    "        model_results['best score'] = gs.best_score_\n",
    "        model_results['confusion matrix']={\"TP\": tp,\"FP\":fp, \"TN\": tn, \"FN\": fn}\n",
    "        model_results['train accuracy'] = gs.score(X_train, y_train)\n",
    "        model_results['test accuracy'] = gs.score(X_test, y_test)\n",
    "        model_results['baseline accuracy'] = 0.5166\n",
    "        \n",
    "        model_results['specificity']= tn/(tn+fp)  \n",
    "        model_results['f1-score']= classi_dict['weighted avg']['f1-score']\n",
    "\n",
    "\n",
    "        # add data\n",
    "        df_list.append(model_results) \n",
    "        pd.set_option(\"display.max_colwidth\", 200)\n",
    "    return (pd.DataFrame(df_list)).round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfc_xnCYCwHG",
    "outputId": "48ebf95b-6b66-4d67-e5b2-8d011cfc2a30"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall (sensitivity)</th>\n",
       "      <th>best params</th>\n",
       "      <th>best score</th>\n",
       "      <th>confusion matrix</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>baseline accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cvec+ multi_nb</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cvec + ss + knn</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>{'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  AUC Score  precision  recall (sensitivity)  \\\n",
       "0      cvec+ multi_nb       0.72       0.67                  0.67   \n",
       "1     cvec + ss + knn       0.60       0.58                  0.58   \n",
       "2  cvec + ss + logreg       0.73       0.69                  0.69   \n",
       "\n",
       "                                                                                                            best params  \\\n",
       "0  {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "1  {'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "2  {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "\n",
       "   best score                              confusion matrix  train accuracy  \\\n",
       "0        0.65    {'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}            0.68   \n",
       "1        0.60  {'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}            0.72   \n",
       "2        0.65    {'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}            0.69   \n",
       "\n",
       "   test accuracy  baseline accuracy  specificity  f1-score  \n",
       "0           0.67               0.52         0.69      0.67  \n",
       "1           0.58               0.52         0.57      0.58  \n",
       "2           0.69               0.52         0.67      0.69  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing function with count vectorizor\n",
    "\n",
    "df_list=[]\n",
    "\n",
    "# models\n",
    "steps_titles = ['cvec+ multi_nb','cvec + ss + knn','cvec + ss + logreg']\n",
    "\n",
    "# pipelines\n",
    "steps_list = [ \n",
    "    [('cv', CountVectorizer()),('multi_nb', MultinomialNB())],\n",
    "    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]\n",
    "\n",
    "# parameters\n",
    "pipe_params = [\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]},\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]},\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]}\n",
    "]   \n",
    "\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9m0NInVCCwHL"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall (sensitivity)</th>\n",
       "      <th>best params</th>\n",
       "      <th>best score</th>\n",
       "      <th>confusion matrix</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>baseline accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cvec+ multi_nb</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cvec + ss + knn</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>{'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tvec + multi_nb</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tvec + ss + knn</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  AUC Score  precision  recall (sensitivity)  \\\n",
       "0      cvec+ multi_nb       0.72       0.67                  0.67   \n",
       "1     cvec + ss + knn       0.60       0.58                  0.58   \n",
       "2  cvec + ss + logreg       0.73       0.69                  0.69   \n",
       "3     tvec + multi_nb       0.73       0.68                  0.68   \n",
       "4     tvec + ss + knn       0.56       0.54                  0.54   \n",
       "5  tvec + ss + logreg       0.73       0.67                  0.67   \n",
       "\n",
       "                                                                                                            best params  \\\n",
       "0  {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "1  {'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "2  {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "3  {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "4  {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n",
       "5  {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "\n",
       "   best score                              confusion matrix  train accuracy  \\\n",
       "0        0.65    {'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}            0.68   \n",
       "1        0.60  {'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}            0.72   \n",
       "2        0.65    {'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}            0.69   \n",
       "3        0.65    {'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}            0.68   \n",
       "4        0.60   {'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}            0.74   \n",
       "5        0.65    {'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}            0.68   \n",
       "\n",
       "   test accuracy  baseline accuracy  specificity  f1-score  \n",
       "0           0.67               0.52         0.69      0.67  \n",
       "1           0.58               0.52         0.57      0.58  \n",
       "2           0.69               0.52         0.67      0.69  \n",
       "3           0.68               0.52         0.67      0.68  \n",
       "4           0.54               0.52         0.49      0.54  \n",
       "5           0.67               0.52         0.69      0.67  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing function with TFD Vectorizor\n",
    "\n",
    "# models\n",
    "steps_titles = ['tvec + multi_nb','tvec + ss + knn','tvec + ss + logreg']\n",
    "\n",
    "# pipelines\n",
    "steps_list = [ \n",
    "    [('tv', TfidfVectorizer()),('multi_nb', MultinomialNB())],\n",
    "    [('tv', TfidfVectorizer()),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('tv', TfidfVectorizer()),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]\n",
    "\n",
    "# parameters\n",
    "pipe_params = [\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]},\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]},\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]}\n",
    "]   \n",
    "\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vpqaKLeMCwHP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall (sensitivity)</th>\n",
       "      <th>best params</th>\n",
       "      <th>best score</th>\n",
       "      <th>confusion matrix</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>baseline accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cvec+ multi_nb</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cvec + ss + knn</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>{'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tvec + multi_nb</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tvec + ss + knn</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hvec + multi_nb</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>{'TP': 148, 'FP': 54, 'TN': 176, 'FN': 97}</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hvec + ss + knn</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.52</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.52</td>\n",
       "      <td>{'TP': 245, 'FP': 229, 'TN': 1, 'FN': 0}</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hvec + ss + logreg</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.63</td>\n",
       "      <td>{'TP': 155, 'FP': 90, 'TN': 140, 'FN': 90}</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  AUC Score  precision  recall (sensitivity)  \\\n",
       "0      cvec+ multi_nb       0.72       0.67                  0.67   \n",
       "1     cvec + ss + knn       0.60       0.58                  0.58   \n",
       "2  cvec + ss + logreg       0.73       0.69                  0.69   \n",
       "3     tvec + multi_nb       0.73       0.68                  0.68   \n",
       "4     tvec + ss + knn       0.56       0.54                  0.54   \n",
       "5  tvec + ss + logreg       0.73       0.67                  0.67   \n",
       "6     hvec + multi_nb       0.77       0.69                  0.68   \n",
       "7     hvec + ss + knn       0.51       0.75                  0.52   \n",
       "8  hvec + ss + logreg       0.65       0.62                  0.62   \n",
       "\n",
       "                                                                                                            best params  \\\n",
       "0  {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "1  {'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "2  {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "3  {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "4  {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n",
       "5  {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "6                                                              {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "7                                                              {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "8                                                              {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "\n",
       "   best score                              confusion matrix  train accuracy  \\\n",
       "0        0.65    {'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}            0.68   \n",
       "1        0.60  {'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}            0.72   \n",
       "2        0.65    {'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}            0.69   \n",
       "3        0.65    {'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}            0.68   \n",
       "4        0.60   {'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}            0.74   \n",
       "5        0.65    {'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}            0.68   \n",
       "6        0.72    {'TP': 148, 'FP': 54, 'TN': 176, 'FN': 97}            0.89   \n",
       "7        0.52      {'TP': 245, 'FP': 229, 'TN': 1, 'FN': 0}            0.52   \n",
       "8        0.63    {'TP': 155, 'FP': 90, 'TN': 140, 'FN': 90}            1.00   \n",
       "\n",
       "   test accuracy  baseline accuracy  specificity  f1-score  \n",
       "0           0.67               0.52         0.69      0.67  \n",
       "1           0.58               0.52         0.57      0.58  \n",
       "2           0.69               0.52         0.67      0.69  \n",
       "3           0.68               0.52         0.67      0.68  \n",
       "4           0.54               0.52         0.49      0.54  \n",
       "5           0.67               0.52         0.69      0.67  \n",
       "6           0.68               0.52         0.77      0.68  \n",
       "7           0.52               0.52         0.00      0.36  \n",
       "8           0.62               0.52         0.61      0.62  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing function with Hashing Vectorizor\n",
    "\n",
    "# models\n",
    "steps_titles = ['hvec + multi_nb','hvec + ss + knn','hvec + ss + logreg']\n",
    "\n",
    "# pipelines\n",
    "steps_list = [ \n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('multi_nb', MultinomialNB())],\n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]\n",
    "\n",
    "# parameters\n",
    "pipe_params = [\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]},\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]},\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]}\n",
    "]   \n",
    "\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WlhF5QNDCwHS"
   },
   "source": [
    "After reviewing all the performances, two models stand out. The Hashing Vectorizor and Multinomial Naive Bayes model had much better metrics, along with the TFID Vectorizor and Multinomial Naive Bayes Model. They both have good AUC values and high generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XIyckvRyCwHU"
   },
   "source": [
    "### Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibkImEdmCwHV"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall (sensitivity)</th>\n",
       "      <th>best params</th>\n",
       "      <th>best score</th>\n",
       "      <th>confusion matrix</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>baseline accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cvec+ multi_nb</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cvec + ss + knn</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>{'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tvec + multi_nb</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tvec + ss + knn</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hvec + multi_nb</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>{'TP': 148, 'FP': 54, 'TN': 176, 'FN': 97}</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hvec + ss + knn</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.52</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.52</td>\n",
       "      <td>{'TP': 245, 'FP': 229, 'TN': 1, 'FN': 0}</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hvec + ss + logreg</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.63</td>\n",
       "      <td>{'TP': 155, 'FP': 90, 'TN': 140, 'FN': 90}</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hvec + multi_nb(tuning)</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'hv__n_features': 1000, 'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'TP': 170, 'FP': 75, 'TN': 155, 'FN': 75}</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tvec + multi_nb(tuning)</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'tv__max_df': 0.4, 'tv__max_features': 70, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'TP': 172, 'FP': 76, 'TN': 154, 'FN': 73}</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model  AUC Score  precision  recall (sensitivity)  \\\n",
       "0            cvec+ multi_nb       0.72       0.67                  0.67   \n",
       "1           cvec + ss + knn       0.60       0.58                  0.58   \n",
       "2        cvec + ss + logreg       0.73       0.69                  0.69   \n",
       "3           tvec + multi_nb       0.73       0.68                  0.68   \n",
       "4           tvec + ss + knn       0.56       0.54                  0.54   \n",
       "5        tvec + ss + logreg       0.73       0.67                  0.67   \n",
       "6           hvec + multi_nb       0.77       0.69                  0.68   \n",
       "7           hvec + ss + knn       0.51       0.75                  0.52   \n",
       "8        hvec + ss + logreg       0.65       0.62                  0.62   \n",
       "9   hvec + multi_nb(tuning)       0.75       0.68                  0.68   \n",
       "10  tvec + multi_nb(tuning)       0.75       0.69                  0.69   \n",
       "\n",
       "                                                                                                             best params  \\\n",
       "0   {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "1   {'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "2   {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "3   {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "4   {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n",
       "5   {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "6                                                               {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "7                                                               {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "8                                                               {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "9                                       {'hv__n_features': 1000, 'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "10  {'tv__max_df': 0.4, 'tv__max_features': 70, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "\n",
       "    best score                              confusion matrix  train accuracy  \\\n",
       "0         0.65    {'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}            0.68   \n",
       "1         0.60  {'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}            0.72   \n",
       "2         0.65    {'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}            0.69   \n",
       "3         0.65    {'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}            0.68   \n",
       "4         0.60   {'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}            0.74   \n",
       "5         0.65    {'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}            0.68   \n",
       "6         0.72    {'TP': 148, 'FP': 54, 'TN': 176, 'FN': 97}            0.89   \n",
       "7         0.52      {'TP': 245, 'FP': 229, 'TN': 1, 'FN': 0}            0.52   \n",
       "8         0.63    {'TP': 155, 'FP': 90, 'TN': 140, 'FN': 90}            1.00   \n",
       "9         0.69    {'TP': 170, 'FP': 75, 'TN': 155, 'FN': 75}            0.82   \n",
       "10        0.68    {'TP': 172, 'FP': 76, 'TN': 154, 'FN': 73}            0.71   \n",
       "\n",
       "    test accuracy  baseline accuracy  specificity  f1-score  \n",
       "0            0.67               0.52         0.69      0.67  \n",
       "1            0.58               0.52         0.57      0.58  \n",
       "2            0.69               0.52         0.67      0.69  \n",
       "3            0.68               0.52         0.67      0.68  \n",
       "4            0.54               0.52         0.49      0.54  \n",
       "5            0.67               0.52         0.69      0.67  \n",
       "6            0.68               0.52         0.77      0.68  \n",
       "7            0.52               0.52         0.00      0.36  \n",
       "8            0.62               0.52         0.61      0.62  \n",
       "9            0.68               0.52         0.67      0.68  \n",
       "10           0.69               0.52         0.67      0.69  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the function for out two narrowed models\n",
    "\n",
    "# models\n",
    "steps_titles = ['hvec + multi_nb(tuning)','tvec + multi_nb(tuning)']\n",
    "\n",
    "# pipelines\n",
    "steps_list = [ \n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('multi_nb', MultinomialNB())],\n",
    "    [('tv', TfidfVectorizer()),('multi_nb', MultinomialNB())]\n",
    "]\n",
    "\n",
    "# parameters\n",
    "pipe_params = [\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2),(1,3)], 'hv__n_features': [50, 150, 300, 500, 800, 1000]},\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2),(1,3)],'tv__max_features': [20, 30, 50, 70, 100],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3, .35,.4]},\n",
    "]   \n",
    "\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6iILFOuKCwHa"
   },
   "source": [
    "### TF-IDF Vectorizor + Multinomial Naive Bayes\n",
    "This model has the best metrics and will work the best for our data. While it isn't the higher AUC, it generalizes better and has higher True Positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "20Dxs2deCwHb"
   },
   "source": [
    "## Running our Optimized Model\n",
    "This model is a combination of TF-IDF(\"Term Frequency - Inverse Document\" Frequency) Vectorizer and the Multinomial Naive Bayes. It assigns scores for the top 70 words in our selected feature. TF-IDF will penalize common words, helping the model find specific key words. The model makes a prediction based on a matrix of word scores and gives a probability of falling into a certain classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qj2GpgECwHh"
   },
   "outputs": [],
   "source": [
    "# getting ready for training\n",
    "\n",
    "X = model_data[\"selftext_clean\"]\n",
    "y = model_data['is_suicide']\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "tvec_optimised = TfidfVectorizer(max_df= 0.5, max_features=70, min_df=2, ngram_range=(1, 3),stop_words = 'english')\n",
    "X_train_tvec = tvec_optimised.fit_transform(X_train).todense()\n",
    "X_test_tvec = tvec_optimised.transform(X_test).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "vocabulary_size = 5000\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1453 samples, validate on 64 samples\n",
      "Epoch 1/10\n",
      "1453/1453 [==============================] - 14s 10ms/step - loss: 0.6871 - accuracy: 0.5396 - val_loss: 0.6753 - val_accuracy: 0.6094\n",
      "Epoch 2/10\n",
      "1453/1453 [==============================] - 11s 7ms/step - loss: 0.6707 - accuracy: 0.5788 - val_loss: 0.6447 - val_accuracy: 0.7188\n",
      "Epoch 3/10\n",
      "1453/1453 [==============================] - 11s 8ms/step - loss: 0.6559 - accuracy: 0.6146 - val_loss: 0.5855 - val_accuracy: 0.7812\n",
      "Epoch 4/10\n",
      "1453/1453 [==============================] - 11s 7ms/step - loss: 0.6359 - accuracy: 0.6352 - val_loss: 0.5655 - val_accuracy: 0.7344\n",
      "Epoch 5/10\n",
      "1453/1453 [==============================] - 11s 8ms/step - loss: 0.6257 - accuracy: 0.6469 - val_loss: 0.5437 - val_accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "1453/1453 [==============================] - 11s 7ms/step - loss: 0.6211 - accuracy: 0.6586 - val_loss: 0.5607 - val_accuracy: 0.7188\n",
      "Epoch 7/10\n",
      "1453/1453 [==============================] - 11s 7ms/step - loss: 0.6149 - accuracy: 0.6579 - val_loss: 0.5777 - val_accuracy: 0.7344\n",
      "Epoch 8/10\n",
      "1453/1453 [==============================] - 11s 7ms/step - loss: 0.6143 - accuracy: 0.6586 - val_loss: 0.5440 - val_accuracy: 0.7344\n",
      "Epoch 9/10\n",
      "1453/1453 [==============================] - 12s 8ms/step - loss: 0.6036 - accuracy: 0.6800 - val_loss: 0.5550 - val_accuracy: 0.7031\n",
      "Epoch 10/10\n",
      "1453/1453 [==============================] - 11s 7ms/step - loss: 0.6003 - accuracy: 0.6786 - val_loss: 0.5661 - val_accuracy: 0.6875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13f6bcdd0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
    "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6394736766815186\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5j_YXnVECwHk"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7a683b0e7da4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    221\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                                             self.split)\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \"\"\"\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=70)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8jPFS1GkCwHn"
   },
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 70))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFKhrNzpCwHr"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, 70, weights=[embedding_matrix], input_length=70 , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(keras.layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ywdh_uNUEugV"
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(10, input_dim=70, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vpyU1cB-HDzo"
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(10, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oT1Te39FIDoL"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.add(Dense(10, activation='relu'))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Z_SFCtn3CwHw",
    "outputId": "d113b3e6-cfb2-489e-e9b8-69815dfd6216"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please provide as model targets either a single array or a list of arrays. You passed: y=178     0\n539     0\n673     0\n1269    1\n1367    1\n       ..\n1789    1\n1383    1\n256     0\n152     0\n966     1\nName: is_suicide, Length: 1517, dtype: int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-68fad595eb4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    527\u001b[0m                                          \u001b[0;34m'either a single '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                                          \u001b[0;34m'array or a list of arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m                                          'You passed: y=' + str(y))\n\u001b[0m\u001b[1;32m    530\u001b[0m                 \u001b[0;31m# Typecheck that all inputs are *either* value *or* symbolic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Please provide as model targets either a single array or a list of arrays. You passed: y=178     0\n539     0\n673     0\n1269    1\n1367    1\n       ..\n1789    1\n1383    1\n256     0\n152     0\n966     1\nName: is_suicide, Length: 1517, dtype: int64"
     ]
    }
   ],
   "source": [
    "history = model3.fit(X_train_tvec, y_train, batch_size=32, epochs=100, verbose=1, validation_data=(X_test_tvec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1gBZxUFCwH2"
   },
   "source": [
    "### Model Performace\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
