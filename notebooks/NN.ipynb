{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCfaTGO3CwGL"
   },
   "source": [
    "## Our Model\n",
    "Here we will be creating different models to classify our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "QQF9iwgnCwGN",
    "outputId": "6b3b6265-3000-465f-c741-8f64a047adcf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set configurations\n",
    "pd.set_option('display.max_columns', 100)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# keras imports\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# model imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpxIwRLmCwGb"
   },
   "outputs": [],
   "source": [
    "model_data = pd.read_csv('../data/scheme1.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "colab_type": "code",
    "id": "O50SrCuCCwGf",
    "outputId": "5889f455-04d3-49c8-b308-c51dd5ec66e0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_suicide</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>selftext_length</th>\n",
       "      <th>title_length</th>\n",
       "      <th>megatext_clean</th>\n",
       "      <th>Clustered Labels</th>\n",
       "      <th>New Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Our most-broken and least-understood rules is \"helpers may not invite private contact as a first resort\", so we've made a new wiki to explain it</td>\n",
       "      <td>We understand that most people who reply immediately to an OP with an invitation to talk privately  mean only to help, but this type of response usually leads to either disappointment or disaster....</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/doqwow/our_mostbroken_and_leastunderstood_rules_is/</td>\n",
       "      <td>understand people reply immediately op invitation talk privately mean help type response usually lead either disappointment disaster usually work quite differently say pm anytime casual social con...</td>\n",
       "      <td>broken least understood rule helper may invite private contact first resort made new wiki explain</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>4792</td>\n",
       "      <td>144</td>\n",
       "      <td>sql witch understand people reply immediately op invitation talk privately mean help type response usually lead either disappointment disaster usually work quite differently say pm anytime casual ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Regular Check-In Post</td>\n",
       "      <td>Welcome to /r/depression's check-in post - a place to take a moment and share what is going on and how you are doing. If you have an accomplishment you want to talk about (these shouldn't be stand...</td>\n",
       "      <td>circinia</td>\n",
       "      <td>1644</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/exo6f1/regular_checkin_post/</td>\n",
       "      <td>welcome r depression check post place take moment share going accomplishment want talk standalone post sub violate role model rule welcome tough time prefer make post place share try best keep spa...</td>\n",
       "      <td>regular check post</td>\n",
       "      <td>c irc</td>\n",
       "      <td>650</td>\n",
       "      <td>21</td>\n",
       "      <td>c irc welcome r depression check post place take moment share going accomplishment want talk standalone post sub violate role model rule welcome tough time prefer make post place share try best ke...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>I hate it so much when you try and express your feelings to your parents, but they turn it around and compare your suffering with theirs.</td>\n",
       "      <td>I've been feeling really depressed and lonely lately from my job, I'm a full time late night janitor for a courthouse just 10 miles away from my hometown. Working there has been pretty easy, but I...</td>\n",
       "      <td>TheNewKiller69</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/fedwbi/i_hate_it_so_much_when_you_try_and_express_your/</td>\n",
       "      <td>feeling really depressed lonely lately job full time late night janitor courthouse 10 mile away hometown working ha pretty easy wound feeling super lonely lot considering really get talk friend ha...</td>\n",
       "      <td>hate much try express feeling parent turn around compare suffering</td>\n",
       "      <td>new killer 69</td>\n",
       "      <td>1866</td>\n",
       "      <td>137</td>\n",
       "      <td>new killer 69 feeling really depressed lonely lately job full time late night janitor courthouse 10 mile away hometown working ha pretty easy wound feeling super lonely lot considering really get ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             0   \n",
       "1           1             1   \n",
       "2           2             2   \n",
       "\n",
       "                                                                                                                                              title  \\\n",
       "0  Our most-broken and least-understood rules is \"helpers may not invite private contact as a first resort\", so we've made a new wiki to explain it   \n",
       "1                                                                                                                             Regular Check-In Post   \n",
       "2         I hate it so much when you try and express your feelings to your parents, but they turn it around and compare your suffering with theirs.   \n",
       "\n",
       "                                                                                                                                                                                                  selftext  \\\n",
       "0  We understand that most people who reply immediately to an OP with an invitation to talk privately  mean only to help, but this type of response usually leads to either disappointment or disaster....   \n",
       "1  Welcome to /r/depression's check-in post - a place to take a moment and share what is going on and how you are doing. If you have an accomplishment you want to talk about (these shouldn't be stand...   \n",
       "2  I've been feeling really depressed and lonely lately from my job, I'm a full time late night janitor for a courthouse just 10 miles away from my hometown. Working there has been pretty easy, but I...   \n",
       "\n",
       "           author  num_comments  is_suicide  \\\n",
       "0        SQLwitch           133           0   \n",
       "1        circinia          1644           0   \n",
       "2  TheNewKiller69             8           0   \n",
       "\n",
       "                                                                                                    url  \\\n",
       "0      https://www.reddit.com/r/depression/comments/doqwow/our_mostbroken_and_leastunderstood_rules_is/   \n",
       "1                             https://www.reddit.com/r/depression/comments/exo6f1/regular_checkin_post/   \n",
       "2  https://www.reddit.com/r/depression/comments/fedwbi/i_hate_it_so_much_when_you_try_and_express_your/   \n",
       "\n",
       "                                                                                                                                                                                            selftext_clean  \\\n",
       "0  understand people reply immediately op invitation talk privately mean help type response usually lead either disappointment disaster usually work quite differently say pm anytime casual social con...   \n",
       "1  welcome r depression check post place take moment share going accomplishment want talk standalone post sub violate role model rule welcome tough time prefer make post place share try best keep spa...   \n",
       "2  feeling really depressed lonely lately job full time late night janitor courthouse 10 mile away hometown working ha pretty easy wound feeling super lonely lot considering really get talk friend ha...   \n",
       "\n",
       "                                                                                         title_clean  \\\n",
       "0  broken least understood rule helper may invite private contact first resort made new wiki explain   \n",
       "1                                                                                 regular check post   \n",
       "2                                 hate much try express feeling parent turn around compare suffering   \n",
       "\n",
       "    author_clean  selftext_length  title_length  \\\n",
       "0      sql witch             4792           144   \n",
       "1          c irc              650            21   \n",
       "2  new killer 69             1866           137   \n",
       "\n",
       "                                                                                                                                                                                            megatext_clean  \\\n",
       "0  sql witch understand people reply immediately op invitation talk privately mean help type response usually lead either disappointment disaster usually work quite differently say pm anytime casual ...   \n",
       "1  c irc welcome r depression check post place take moment share going accomplishment want talk standalone post sub violate role model rule welcome tough time prefer make post place share try best ke...   \n",
       "2  new killer 69 feeling really depressed lonely lately job full time late night janitor courthouse 10 mile away hometown working ha pretty easy wound feeling super lonely lot considering really get ...   \n",
       "\n",
       "   Clustered Labels  New Labels  \n",
       "0                 1           1  \n",
       "1                 1           1  \n",
       "2                 0           0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "GJA-54VWCwGk",
    "outputId": "f08f2be6-8d4e-4c26-ad00-dab8d04745a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1897 entries, 0 to 1896\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Unnamed: 0        1897 non-null   int64 \n",
      " 1   Unnamed: 0.1      1897 non-null   int64 \n",
      " 2   title             1897 non-null   object\n",
      " 3   selftext          1897 non-null   object\n",
      " 4   author            1897 non-null   object\n",
      " 5   num_comments      1897 non-null   int64 \n",
      " 6   is_suicide        1897 non-null   int64 \n",
      " 7   url               1897 non-null   object\n",
      " 8   selftext_clean    1897 non-null   object\n",
      " 9   title_clean       1897 non-null   object\n",
      " 10  author_clean      1897 non-null   object\n",
      " 11  selftext_length   1897 non-null   int64 \n",
      " 12  title_length      1897 non-null   int64 \n",
      " 13  megatext_clean    1897 non-null   object\n",
      " 14  Clustered Labels  1897 non-null   int64 \n",
      " 15  New Labels        1897 non-null   int64 \n",
      "dtypes: int64(8), object(8)\n",
      "memory usage: 237.2+ KB\n"
     ]
    }
   ],
   "source": [
    "model_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rB4_ZoKPCwGq"
   },
   "source": [
    "Establishing a baseline accuracy is important for evaluating the model's progression. If every prediction was 1, let's see what our accuracy would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nWfHHM4VCwGr",
    "outputId": "6eecaa03-d095-44ac-e9de-490e28bdb2ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5166051660516605"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data['is_suicide'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PfgxVAnrCwGw"
   },
   "source": [
    "Our baseline accuracy is about 51.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awnyv7ftCwGx"
   },
   "source": [
    "### Selection of Features and Attributes\n",
    "Here we are attempting to create a model using CountVectorizer and Naive Bayes Model to determine which columns are the best to score and use for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qM_aEmPmCwGx"
   },
   "outputs": [],
   "source": [
    "# Creating a function to score different models from different columsn in the dataset\n",
    "\n",
    "columns_list = ['column_1', \"column_2\", \"column_3\"]\n",
    "model = \"CountVec + MultinomialNB\"\n",
    "df_list=[] \n",
    "\n",
    "def multi_modelling(columns_list, model):\n",
    "    for i in columns_list:\n",
    "        \n",
    "        X = model_data[i]\n",
    "        y = model_data['is_suicide']\n",
    "        \n",
    "        # train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "        \n",
    "        cvec = CountVectorizer()\n",
    "        cvec.fit(X_train)\n",
    "        \n",
    "        # creating dataframes\n",
    "        X_train = pd.DataFrame(cvec.transform(X_train).todense(),\n",
    "                               columns=cvec.get_feature_names())\n",
    "        X_test = pd.DataFrame(cvec.transform(X_test).todense(),\n",
    "                               columns=cvec.get_feature_names())\n",
    "        \n",
    "        nb = MultinomialNB()\n",
    "        nb.fit(X_train,y_train)\n",
    "        \n",
    "        # get predictions from model\n",
    "        pred = nb.predict(X_test)\n",
    "        \n",
    "        # create confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "        \n",
    "        # calculating AUC\n",
    "        nb.predict_proba(X_test)\n",
    "        pred_proba = [i[1] for i in nb.predict_proba(X_test)] \n",
    "        auc = roc_auc_score(y_test, pred_proba)\n",
    "\n",
    "        # classification report\n",
    "        classi_dict = (classification_report(y_test,pred, output_dict=True))\n",
    "\n",
    "        # storing our results\n",
    "        model_results = {}\n",
    "        model_results['series used (X)'] = i\n",
    "        model_results['model'] = model\n",
    "        model_results['AUC Score'] = auc\n",
    "        model_results['precision']= classi_dict['weighted avg']['precision']\n",
    "        model_results['recall (sensitivity)']= classi_dict['weighted avg']['recall']\n",
    "        model_results['confusion matrix']={\"TP\": tp,\"FP\":fp, \"TN\": tn, \"FN\": fn}\n",
    "        model_results['train accuracy'] = nb.score(X_train, y_train)\n",
    "        model_results['test accuracy'] = nb.score(X_test, y_test)\n",
    "        model_results['baseline accuracy']=0.5166\n",
    "        model_results['specificity']= tn/(tn+fp)  \n",
    "        model_results['f1-score']= classi_dict['weighted avg']['f1-score']\n",
    "        #model_results['support']= classi_dict['weighted avg']['support']\n",
    "        model_results\n",
    "        df_list.append(model_results) \n",
    "\n",
    "    pd.set_option(\"display.max_colwidth\", 50)\n",
    "    return (pd.DataFrame(df_list)).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jKlVOkfiCwG5"
   },
   "source": [
    "For our initial model, it will be a binary classifier. Once the user gets their classification, they will go to another model that will give them specific support. \n",
    "\n",
    "Label Encoding: r/SuicideWatch = 1, r/Depression = 0\n",
    "\n",
    "TP: model predicts suicide, and it is correct\n",
    "\n",
    "TN: model predicts depression, and it is correct\n",
    "\n",
    "FP: model predicts suicide, but it is really depression, not good\n",
    "\n",
    "FN: model predicts depression, but they really are suicidal, this is the worst, misses an at risk patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "JmxyjjuICwG6",
    "outputId": "840793c4-70ef-4b6e-86e3-86ca18dc8c33"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series used (X)</th>\n",
       "      <th>model</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall (sensitivity)</th>\n",
       "      <th>confusion matrix</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>baseline accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>selftext</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>{'TP': 161, 'FP': 78, 'TN': 152, 'FN': 84}</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>author</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.55</td>\n",
       "      <td>{'TP': 235, 'FP': 204, 'TN': 26, 'FN': 10}</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>title</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'TP': 167, 'FP': 104, 'TN': 126, 'FN': 78}</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>selftext_clean</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'TP': 165, 'FP': 78, 'TN': 152, 'FN': 80}</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>author_clean</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.51</td>\n",
       "      <td>{'TP': 169, 'FP': 155, 'TN': 75, 'FN': 76}</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>title_clean</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'TP': 178, 'FP': 112, 'TN': 118, 'FN': 67}</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>megatext_clean</td>\n",
       "      <td>CountVec + MultinomialNB</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  series used (X)                     model  AUC Score  precision  \\\n",
       "0        selftext  CountVec + MultinomialNB       0.69       0.66   \n",
       "1          author  CountVec + MultinomialNB       0.57       0.63   \n",
       "2           title  CountVec + MultinomialNB       0.67       0.62   \n",
       "3  selftext_clean  CountVec + MultinomialNB       0.69       0.67   \n",
       "4    author_clean  CountVec + MultinomialNB       0.54       0.51   \n",
       "5     title_clean  CountVec + MultinomialNB       0.67       0.63   \n",
       "6  megatext_clean  CountVec + MultinomialNB       0.71       0.67   \n",
       "\n",
       "   recall (sensitivity)                             confusion matrix  \\\n",
       "0                  0.66   {'TP': 161, 'FP': 78, 'TN': 152, 'FN': 84}   \n",
       "1                  0.55   {'TP': 235, 'FP': 204, 'TN': 26, 'FN': 10}   \n",
       "2                  0.62  {'TP': 167, 'FP': 104, 'TN': 126, 'FN': 78}   \n",
       "3                  0.67   {'TP': 165, 'FP': 78, 'TN': 152, 'FN': 80}   \n",
       "4                  0.51   {'TP': 169, 'FP': 155, 'TN': 75, 'FN': 76}   \n",
       "5                  0.62  {'TP': 178, 'FP': 112, 'TN': 118, 'FN': 67}   \n",
       "6                  0.67   {'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}   \n",
       "\n",
       "   train accuracy  test accuracy  baseline accuracy  specificity  f1-score  \n",
       "0            0.92           0.66               0.52         0.66      0.66  \n",
       "1            0.99           0.55               0.52         0.11      0.45  \n",
       "2            0.85           0.62               0.52         0.55      0.62  \n",
       "3            0.91           0.67               0.52         0.66      0.67  \n",
       "4            0.95           0.51               0.52         0.33      0.50  \n",
       "5            0.84           0.62               0.52         0.51      0.62  \n",
       "6            0.95           0.67               0.52         0.69      0.67  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the function\n",
    "columns_list = ['selftext', \"author\", \"title\",'selftext_clean', \"author_clean\", \"title_clean\", \"megatext_clean\"]\n",
    "model = \"CountVec + MultinomialNB\"\n",
    "df_list=[]\n",
    "multi_modelling(columns_list, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RYGi-Jq5CwG_"
   },
   "source": [
    "Based on our results, using the megatext_clean column will be the best option for our model due its good metrics, such as its AUC value. The false negatives are the highest in our confusion matrix, meaning it is good for the model to learn from. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jAU0IhOdCwHB"
   },
   "source": [
    "### Production Model\n",
    "We will test a few different models by running some perumatations and recording the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvR0cpEACwHC"
   },
   "outputs": [],
   "source": [
    "# Defining a function that test different models and record parameters\n",
    "\n",
    "def gridsearch_multi(steps_titles, steps_list, pipe_params):\n",
    "    \n",
    "    X = model_data[\"megatext_clean\"]\n",
    "    y = model_data['is_suicide']\n",
    "    \n",
    "    # train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "    # build dataframe\n",
    "    gs_results = pd.DataFrame(columns=['model','AUC Score', 'precision', 'recall (sensitivity)', \n",
    "                                       'best_params', 'best score', 'confusion matrix', \n",
    "                                       'train_accuracy','test_accuracy','baseline_accuracy',\n",
    "                                       'specificity', 'f1-score'])\n",
    "\n",
    "    for i in range(len(steps_list)):\n",
    "        # define pipeline\n",
    "        pipe = Pipeline(steps=steps_list[i])\n",
    "\n",
    "        gs = GridSearchCV(pipe, pipe_params[i], cv=3) \n",
    "        gs.fit(X_train, y_train)\n",
    "        \n",
    "        # getting predictions from model\n",
    "        pred = gs.predict(X_test)\n",
    "        \n",
    "        # confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, gs.predict(X_test)).ravel() \n",
    "        \n",
    "        classi_dict = (classification_report(y_test,pred, output_dict=True))\n",
    "        \n",
    "        # calculating AUC\n",
    "        gs.predict_proba(X_test)\n",
    "        pred_proba = [i[1] for i in gs.predict_proba(X_test)] \n",
    "        auc = roc_auc_score(y_test, pred_proba)\n",
    "        \n",
    "        # create dataframe columns\n",
    "        model_results = {}\n",
    "        model_results['model'] = steps_titles[i]\n",
    "        model_results['AUC Score'] = auc\n",
    "        model_results['precision']= classi_dict['weighted avg']['precision']\n",
    "        model_results['recall (sensitivity)']= classi_dict['weighted avg']['recall']\n",
    "        model_results['best params'] = gs.best_params_\n",
    "        model_results['best score'] = gs.best_score_\n",
    "        model_results['confusion matrix']={\"TP\": tp,\"FP\":fp, \"TN\": tn, \"FN\": fn}\n",
    "        model_results['train accuracy'] = gs.score(X_train, y_train)\n",
    "        model_results['test accuracy'] = gs.score(X_test, y_test)\n",
    "        model_results['baseline accuracy'] = 0.5166\n",
    "        \n",
    "        model_results['specificity']= tn/(tn+fp)  \n",
    "        model_results['f1-score']= classi_dict['weighted avg']['f1-score']\n",
    "\n",
    "\n",
    "        # add data\n",
    "        df_list.append(model_results) \n",
    "        pd.set_option(\"display.max_colwidth\", 200)\n",
    "    return (pd.DataFrame(df_list)).round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfc_xnCYCwHG",
    "outputId": "48ebf95b-6b66-4d67-e5b2-8d011cfc2a30"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall (sensitivity)</th>\n",
       "      <th>best params</th>\n",
       "      <th>best score</th>\n",
       "      <th>confusion matrix</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>baseline accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cvec+ multi_nb</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cvec + ss + knn</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>{'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  AUC Score  precision  recall (sensitivity)  \\\n",
       "0      cvec+ multi_nb       0.72       0.67                  0.67   \n",
       "1     cvec + ss + knn       0.60       0.58                  0.58   \n",
       "2  cvec + ss + logreg       0.73       0.69                  0.69   \n",
       "\n",
       "                                                                                                            best params  \\\n",
       "0  {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "1  {'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "2  {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "\n",
       "   best score                              confusion matrix  train accuracy  \\\n",
       "0        0.65    {'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}            0.68   \n",
       "1        0.60  {'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}            0.72   \n",
       "2        0.65    {'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}            0.69   \n",
       "\n",
       "   test accuracy  baseline accuracy  specificity  f1-score  \n",
       "0           0.67               0.52         0.69      0.67  \n",
       "1           0.58               0.52         0.57      0.58  \n",
       "2           0.69               0.52         0.67      0.69  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing function with count vectorizor\n",
    "\n",
    "df_list=[]\n",
    "\n",
    "# models\n",
    "steps_titles = ['cvec+ multi_nb','cvec + ss + knn','cvec + ss + logreg']\n",
    "\n",
    "# pipelines\n",
    "steps_list = [ \n",
    "    [('cv', CountVectorizer()),('multi_nb', MultinomialNB())],\n",
    "    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]\n",
    "\n",
    "# parameters\n",
    "pipe_params = [\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]},\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]},\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)],'cv__max_features': [20, 30, 50],'cv__min_df': [2, 3],'cv__max_df': [.2, .25, .3]}\n",
    "]   \n",
    "\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9m0NInVCCwHL"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall (sensitivity)</th>\n",
       "      <th>best params</th>\n",
       "      <th>best score</th>\n",
       "      <th>confusion matrix</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>baseline accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cvec+ multi_nb</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cvec + ss + knn</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>{'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tvec + multi_nb</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tvec + ss + knn</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  AUC Score  precision  recall (sensitivity)  \\\n",
       "0      cvec+ multi_nb       0.72       0.67                  0.67   \n",
       "1     cvec + ss + knn       0.60       0.58                  0.58   \n",
       "2  cvec + ss + logreg       0.73       0.69                  0.69   \n",
       "3     tvec + multi_nb       0.73       0.68                  0.68   \n",
       "4     tvec + ss + knn       0.56       0.54                  0.54   \n",
       "5  tvec + ss + logreg       0.73       0.67                  0.67   \n",
       "\n",
       "                                                                                                            best params  \\\n",
       "0  {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "1  {'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "2  {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "3  {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "4  {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n",
       "5  {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "\n",
       "   best score                              confusion matrix  train accuracy  \\\n",
       "0        0.65    {'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}            0.68   \n",
       "1        0.60  {'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}            0.72   \n",
       "2        0.65    {'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}            0.69   \n",
       "3        0.65    {'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}            0.68   \n",
       "4        0.60   {'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}            0.74   \n",
       "5        0.65    {'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}            0.68   \n",
       "\n",
       "   test accuracy  baseline accuracy  specificity  f1-score  \n",
       "0           0.67               0.52         0.69      0.67  \n",
       "1           0.58               0.52         0.57      0.58  \n",
       "2           0.69               0.52         0.67      0.69  \n",
       "3           0.68               0.52         0.67      0.68  \n",
       "4           0.54               0.52         0.49      0.54  \n",
       "5           0.67               0.52         0.69      0.67  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing function with TFD Vectorizor\n",
    "\n",
    "# models\n",
    "steps_titles = ['tvec + multi_nb','tvec + ss + knn','tvec + ss + logreg']\n",
    "\n",
    "# pipelines\n",
    "steps_list = [ \n",
    "    [('tv', TfidfVectorizer()),('multi_nb', MultinomialNB())],\n",
    "    [('tv', TfidfVectorizer()),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('tv', TfidfVectorizer()),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]\n",
    "\n",
    "# parameters\n",
    "pipe_params = [\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]},\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]},\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2)],'tv__max_features': [20, 30, 50],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3]}\n",
    "]   \n",
    "\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vpqaKLeMCwHP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall (sensitivity)</th>\n",
       "      <th>best params</th>\n",
       "      <th>best score</th>\n",
       "      <th>confusion matrix</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>baseline accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cvec+ multi_nb</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cvec + ss + knn</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>{'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tvec + multi_nb</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tvec + ss + knn</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hvec + multi_nb</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>{'TP': 148, 'FP': 54, 'TN': 176, 'FN': 97}</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hvec + ss + knn</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.52</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.52</td>\n",
       "      <td>{'TP': 245, 'FP': 229, 'TN': 1, 'FN': 0}</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hvec + ss + logreg</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.63</td>\n",
       "      <td>{'TP': 155, 'FP': 90, 'TN': 140, 'FN': 90}</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  AUC Score  precision  recall (sensitivity)  \\\n",
       "0      cvec+ multi_nb       0.72       0.67                  0.67   \n",
       "1     cvec + ss + knn       0.60       0.58                  0.58   \n",
       "2  cvec + ss + logreg       0.73       0.69                  0.69   \n",
       "3     tvec + multi_nb       0.73       0.68                  0.68   \n",
       "4     tvec + ss + knn       0.56       0.54                  0.54   \n",
       "5  tvec + ss + logreg       0.73       0.67                  0.67   \n",
       "6     hvec + multi_nb       0.77       0.69                  0.68   \n",
       "7     hvec + ss + knn       0.51       0.75                  0.52   \n",
       "8  hvec + ss + logreg       0.65       0.62                  0.62   \n",
       "\n",
       "                                                                                                            best params  \\\n",
       "0  {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "1  {'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "2  {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "3  {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "4  {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n",
       "5  {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "6                                                              {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "7                                                              {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "8                                                              {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "\n",
       "   best score                              confusion matrix  train accuracy  \\\n",
       "0        0.65    {'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}            0.68   \n",
       "1        0.60  {'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}            0.72   \n",
       "2        0.65    {'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}            0.69   \n",
       "3        0.65    {'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}            0.68   \n",
       "4        0.60   {'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}            0.74   \n",
       "5        0.65    {'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}            0.68   \n",
       "6        0.72    {'TP': 148, 'FP': 54, 'TN': 176, 'FN': 97}            0.89   \n",
       "7        0.52      {'TP': 245, 'FP': 229, 'TN': 1, 'FN': 0}            0.52   \n",
       "8        0.63    {'TP': 155, 'FP': 90, 'TN': 140, 'FN': 90}            1.00   \n",
       "\n",
       "   test accuracy  baseline accuracy  specificity  f1-score  \n",
       "0           0.67               0.52         0.69      0.67  \n",
       "1           0.58               0.52         0.57      0.58  \n",
       "2           0.69               0.52         0.67      0.69  \n",
       "3           0.68               0.52         0.67      0.68  \n",
       "4           0.54               0.52         0.49      0.54  \n",
       "5           0.67               0.52         0.69      0.67  \n",
       "6           0.68               0.52         0.77      0.68  \n",
       "7           0.52               0.52         0.00      0.36  \n",
       "8           0.62               0.52         0.61      0.62  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing function with Hashing Vectorizor\n",
    "\n",
    "# models\n",
    "steps_titles = ['hvec + multi_nb','hvec + ss + knn','hvec + ss + logreg']\n",
    "\n",
    "# pipelines\n",
    "steps_list = [ \n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('multi_nb', MultinomialNB())],\n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]\n",
    "\n",
    "# parameters\n",
    "pipe_params = [\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]},\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]},\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2)]}\n",
    "]   \n",
    "\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WlhF5QNDCwHS"
   },
   "source": [
    "After reviewing all the performances, two models stand out. The Hashing Vectorizor and Multinomial Naive Bayes model had much better metrics, along with the TFID Vectorizor and Multinomial Naive Bayes Model. They both have good AUC values and high generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XIyckvRyCwHU"
   },
   "source": [
    "### Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibkImEdmCwHV"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall (sensitivity)</th>\n",
       "      <th>best params</th>\n",
       "      <th>best score</th>\n",
       "      <th>confusion matrix</th>\n",
       "      <th>train accuracy</th>\n",
       "      <th>test accuracy</th>\n",
       "      <th>baseline accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cvec+ multi_nb</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cvec + ss + knn</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>{'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tvec + multi_nb</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tvec + ss + knn</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>{'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.60</td>\n",
       "      <td>{'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tvec + ss + logreg</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>{'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hvec + multi_nb</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.72</td>\n",
       "      <td>{'TP': 148, 'FP': 54, 'TN': 176, 'FN': 97}</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hvec + ss + knn</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.52</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.52</td>\n",
       "      <td>{'TP': 245, 'FP': 229, 'TN': 1, 'FN': 0}</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hvec + ss + logreg</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.63</td>\n",
       "      <td>{'TP': 155, 'FP': 90, 'TN': 140, 'FN': 90}</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hvec + multi_nb(tuning)</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'hv__n_features': 1000, 'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'TP': 170, 'FP': 75, 'TN': 155, 'FN': 75}</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tvec + multi_nb(tuning)</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>{'tv__max_df': 0.4, 'tv__max_features': 70, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>{'TP': 172, 'FP': 76, 'TN': 154, 'FN': 73}</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model  AUC Score  precision  recall (sensitivity)  \\\n",
       "0            cvec+ multi_nb       0.72       0.67                  0.67   \n",
       "1           cvec + ss + knn       0.60       0.58                  0.58   \n",
       "2        cvec + ss + logreg       0.73       0.69                  0.69   \n",
       "3           tvec + multi_nb       0.73       0.68                  0.68   \n",
       "4           tvec + ss + knn       0.56       0.54                  0.54   \n",
       "5        tvec + ss + logreg       0.73       0.67                  0.67   \n",
       "6           hvec + multi_nb       0.77       0.69                  0.68   \n",
       "7           hvec + ss + knn       0.51       0.75                  0.52   \n",
       "8        hvec + ss + logreg       0.65       0.62                  0.62   \n",
       "9   hvec + multi_nb(tuning)       0.75       0.68                  0.68   \n",
       "10  tvec + multi_nb(tuning)       0.75       0.69                  0.69   \n",
       "\n",
       "                                                                                                             best params  \\\n",
       "0   {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "1   {'cv__max_df': 0.2, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "2   {'cv__max_df': 0.3, 'cv__max_features': 50, 'cv__min_df': 2, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}   \n",
       "3   {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "4   {'tv__max_df': 0.2, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 1), 'tv__stop_words': 'english'}   \n",
       "5   {'tv__max_df': 0.3, 'tv__max_features': 50, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "6                                                               {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "7                                                               {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "8                                                               {'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "9                                       {'hv__n_features': 1000, 'hv__ngram_range': (1, 1), 'hv__stop_words': 'english'}   \n",
       "10  {'tv__max_df': 0.4, 'tv__max_features': 70, 'tv__min_df': 2, 'tv__ngram_range': (1, 2), 'tv__stop_words': 'english'}   \n",
       "\n",
       "    best score                              confusion matrix  train accuracy  \\\n",
       "0         0.65    {'TP': 160, 'FP': 71, 'TN': 159, 'FN': 85}            0.68   \n",
       "1         0.60  {'TP': 144, 'FP': 100, 'TN': 130, 'FN': 101}            0.72   \n",
       "2         0.65    {'TP': 173, 'FP': 75, 'TN': 155, 'FN': 72}            0.69   \n",
       "3         0.65    {'TP': 169, 'FP': 77, 'TN': 153, 'FN': 76}            0.68   \n",
       "4         0.60   {'TP': 146, 'FP': 118, 'TN': 112, 'FN': 99}            0.74   \n",
       "5         0.65    {'TP': 160, 'FP': 72, 'TN': 158, 'FN': 85}            0.68   \n",
       "6         0.72    {'TP': 148, 'FP': 54, 'TN': 176, 'FN': 97}            0.89   \n",
       "7         0.52      {'TP': 245, 'FP': 229, 'TN': 1, 'FN': 0}            0.52   \n",
       "8         0.63    {'TP': 155, 'FP': 90, 'TN': 140, 'FN': 90}            1.00   \n",
       "9         0.69    {'TP': 170, 'FP': 75, 'TN': 155, 'FN': 75}            0.82   \n",
       "10        0.68    {'TP': 172, 'FP': 76, 'TN': 154, 'FN': 73}            0.71   \n",
       "\n",
       "    test accuracy  baseline accuracy  specificity  f1-score  \n",
       "0            0.67               0.52         0.69      0.67  \n",
       "1            0.58               0.52         0.57      0.58  \n",
       "2            0.69               0.52         0.67      0.69  \n",
       "3            0.68               0.52         0.67      0.68  \n",
       "4            0.54               0.52         0.49      0.54  \n",
       "5            0.67               0.52         0.69      0.67  \n",
       "6            0.68               0.52         0.77      0.68  \n",
       "7            0.52               0.52         0.00      0.36  \n",
       "8            0.62               0.52         0.61      0.62  \n",
       "9            0.68               0.52         0.67      0.68  \n",
       "10           0.69               0.52         0.67      0.69  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the function for out two narrowed models\n",
    "\n",
    "# models\n",
    "steps_titles = ['hvec + multi_nb(tuning)','tvec + multi_nb(tuning)']\n",
    "\n",
    "# pipelines\n",
    "steps_list = [ \n",
    "    [('hv', HashingVectorizer(alternate_sign=False)),('multi_nb', MultinomialNB())],\n",
    "    [('tv', TfidfVectorizer()),('multi_nb', MultinomialNB())]\n",
    "]\n",
    "\n",
    "# parameters\n",
    "pipe_params = [\n",
    "    {'hv__stop_words':['english'], 'hv__ngram_range':[(1,1),(1,2),(1,3)], 'hv__n_features': [50, 150, 300, 500, 800, 1000]},\n",
    "    {'tv__stop_words':['english'], 'tv__ngram_range':[(1,1),(1,2),(1,3)],'tv__max_features': [20, 30, 50, 70, 100],'tv__min_df': [2, 3],'tv__max_df': [.2, .25, .3, .35,.4]},\n",
    "]   \n",
    "\n",
    "gridsearch_multi(steps_titles, steps_list, pipe_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6iILFOuKCwHa"
   },
   "source": [
    "### TF-IDF Vectorizor + Multinomial Naive Bayes\n",
    "This model has the best metrics and will work the best for our data. While it isn't the higher AUC, it generalizes better and has higher True Positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "20Dxs2deCwHb"
   },
   "source": [
    "## Running our Optimized Model\n",
    "This model is a combination of TF-IDF(\"Term Frequency - Inverse Document\" Frequency) Vectorizer and the Multinomial Naive Bayes. It assigns scores for the top 70 words in our selected feature. TF-IDF will penalize common words, helping the model find specific key words. The model makes a prediction based on a matrix of word scores and gives a probability of falling into a certain classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qj2GpgECwHh"
   },
   "outputs": [],
   "source": [
    "# getting ready for training\n",
    "\n",
    "X = model_data[\"selftext_clean\"]\n",
    "y = model_data[\"is_suicide\"]\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "tvec_optimised = TfidfVectorizer(max_df= 0.5, max_features=70, min_df=2, ngram_range=(1, 3),stop_words = 'english')\n",
    "X_train_tvec = tvec_optimised.fit_transform(X_train).todense()\n",
    "X_test_tvec = tvec_optimised.transform(X_test).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ' want loose dont know healthy keep seeing hanging mean kinda depressed bad feeling thought come go also worried distance would intensify insecurites love another guy probalby stop seeing tho least to",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-e1c536aa07d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvocabulary_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmax_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ' want loose dont know healthy keep seeing hanging mean kinda depressed bad feeling thought come go also worried distance would intensify insecurites love another guy probalby stop seeing tho least to"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "vocabulary_size = 5000\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1453 samples, validate on 64 samples\n",
      "Epoch 1/10\n",
      "1453/1453 [==============================] - 12s 8ms/step - loss: 0.6847 - accuracy: 0.5272 - val_loss: 0.6629 - val_accuracy: 0.6562\n",
      "Epoch 2/10\n",
      "1453/1453 [==============================] - 11s 7ms/step - loss: 0.6666 - accuracy: 0.5843 - val_loss: 0.6096 - val_accuracy: 0.6875\n",
      "Epoch 3/10\n",
      "1453/1453 [==============================] - 10s 7ms/step - loss: 0.6484 - accuracy: 0.6167 - val_loss: 0.5742 - val_accuracy: 0.7188\n",
      "Epoch 4/10\n",
      "1453/1453 [==============================] - 13s 9ms/step - loss: 0.6385 - accuracy: 0.6339 - val_loss: 0.6069 - val_accuracy: 0.7188\n",
      "Epoch 5/10\n",
      "1453/1453 [==============================] - 11s 8ms/step - loss: 0.6480 - accuracy: 0.6387 - val_loss: 0.5941 - val_accuracy: 0.7188\n",
      "Epoch 6/10\n",
      "1453/1453 [==============================] - 12s 8ms/step - loss: 0.6309 - accuracy: 0.6518 - val_loss: 0.5468 - val_accuracy: 0.7031\n",
      "Epoch 7/10\n",
      "1453/1453 [==============================] - 11s 8ms/step - loss: 0.6272 - accuracy: 0.6579 - val_loss: 0.5573 - val_accuracy: 0.7188\n",
      "Epoch 8/10\n",
      "1453/1453 [==============================] - 16s 11ms/step - loss: 0.6202 - accuracy: 0.6566 - val_loss: 0.5323 - val_accuracy: 0.7344\n",
      "Epoch 9/10\n",
      "1453/1453 [==============================] - 18s 13ms/step - loss: 0.6182 - accuracy: 0.6600 - val_loss: 0.5348 - val_accuracy: 0.7656\n",
      "Epoch 10/10\n",
      "1453/1453 [==============================] - 13s 9ms/step - loss: 0.6152 - accuracy: 0.6538 - val_loss: 0.5358 - val_accuracy: 0.7656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13fcbae50>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "embedding_size= 32\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
    "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.765625\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-f6f5fb507cb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RNN.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "model.save(\"RNN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8jPFS1GkCwHn"
   },
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 70))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFKhrNzpCwHr"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, 70, weights=[embedding_matrix], input_length=70 , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(keras.layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ywdh_uNUEugV"
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(10, input_dim=70, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vpyU1cB-HDzo"
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(10, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oT1Te39FIDoL"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.add(Dense(10, activation='relu'))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 70, 100)           888100    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 63, 32)            25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 31, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 992)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                9930      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 923,673\n",
      "Trainable params: 923,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length= 70))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Z_SFCtn3CwHw",
    "outputId": "d113b3e6-cfb2-489e-e9b8-69815dfd6216"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1517 samples, validate on 380 samples\n",
      "Epoch 1/100\n",
      "1517/1517 [==============================] - 1s 915us/step - loss: 0.6947 - accuracy: 0.5063 - val_loss: 0.6930 - val_accuracy: 0.5158\n",
      "Epoch 2/100\n",
      "1517/1517 [==============================] - 1s 683us/step - loss: 0.6929 - accuracy: 0.5168 - val_loss: 0.6928 - val_accuracy: 0.5158\n",
      "Epoch 3/100\n",
      "1517/1517 [==============================] - 1s 568us/step - loss: 0.6932 - accuracy: 0.5023 - val_loss: 0.6927 - val_accuracy: 0.5158\n",
      "Epoch 4/100\n",
      "1517/1517 [==============================] - 1s 651us/step - loss: 0.6929 - accuracy: 0.5168 - val_loss: 0.6928 - val_accuracy: 0.5158\n",
      "Epoch 5/100\n",
      "1517/1517 [==============================] - 1s 640us/step - loss: 0.6928 - accuracy: 0.5168 - val_loss: 0.6927 - val_accuracy: 0.5158\n",
      "Epoch 6/100\n",
      "1517/1517 [==============================] - 1s 602us/step - loss: 0.6930 - accuracy: 0.5168 - val_loss: 0.6927 - val_accuracy: 0.5158\n",
      "Epoch 7/100\n",
      "1517/1517 [==============================] - 1s 543us/step - loss: 0.6927 - accuracy: 0.5168 - val_loss: 0.6927 - val_accuracy: 0.5158\n",
      "Epoch 8/100\n",
      "1517/1517 [==============================] - 1s 579us/step - loss: 0.6927 - accuracy: 0.5168 - val_loss: 0.6927 - val_accuracy: 0.5158\n",
      "Epoch 9/100\n",
      "1517/1517 [==============================] - 1s 666us/step - loss: 0.6926 - accuracy: 0.5168 - val_loss: 0.6927 - val_accuracy: 0.5158\n",
      "Epoch 10/100\n",
      "1517/1517 [==============================] - 1s 595us/step - loss: 0.6928 - accuracy: 0.5168 - val_loss: 0.6927 - val_accuracy: 0.5158\n",
      "Epoch 11/100\n",
      "1517/1517 [==============================] - 1s 587us/step - loss: 0.6932 - accuracy: 0.5109 - val_loss: 0.6927 - val_accuracy: 0.5158\n",
      "Epoch 12/100\n",
      "1517/1517 [==============================] - 1s 602us/step - loss: 0.6927 - accuracy: 0.5168 - val_loss: 0.6929 - val_accuracy: 0.5132\n",
      "Epoch 13/100\n",
      "1517/1517 [==============================] - 1s 631us/step - loss: 0.6920 - accuracy: 0.5194 - val_loss: 0.6955 - val_accuracy: 0.5132\n",
      "Epoch 14/100\n",
      "1517/1517 [==============================] - 1s 631us/step - loss: 0.6901 - accuracy: 0.5201 - val_loss: 0.6997 - val_accuracy: 0.5132\n",
      "Epoch 15/100\n",
      "1517/1517 [==============================] - 1s 618us/step - loss: 0.6882 - accuracy: 0.5208 - val_loss: 0.7059 - val_accuracy: 0.5132\n",
      "Epoch 16/100\n",
      "1517/1517 [==============================] - 1s 607us/step - loss: 0.6855 - accuracy: 0.5214 - val_loss: 0.7137 - val_accuracy: 0.5158\n",
      "Epoch 17/100\n",
      "1517/1517 [==============================] - 1s 670us/step - loss: 0.6849 - accuracy: 0.5221 - val_loss: 0.7123 - val_accuracy: 0.5158\n",
      "Epoch 18/100\n",
      "1517/1517 [==============================] - 1s 737us/step - loss: 0.6838 - accuracy: 0.5221 - val_loss: 0.7129 - val_accuracy: 0.5158\n",
      "Epoch 19/100\n",
      " 448/1517 [=======>......................] - ETA: 0s - loss: 0.6828 - accuracy: 0.5201"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-daf1546bf881>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_tvec, y_train, batch_size=32, epochs=100, verbose=1, validation_data=(X_test_tvec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1gBZxUFCwH2"
   },
   "source": [
    "### Model Performace\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
